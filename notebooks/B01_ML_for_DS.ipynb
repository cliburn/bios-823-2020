{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning for Data Scientists\n",
    "\n",
    "I expect that all of you will have taken or are concurrently taking a ML class that focuses on the mathematics and algorithms. Hence this class focuses mostly on practical aspects of ML that are often glossed over in more academic courses, but useful for a practicing data scientist.\n",
    "\n",
    "## A. Understanding ML\n",
    "\n",
    "### A1. What is ML?\n",
    "\n",
    "- Algorithms that get better at performing a task by learning from data\n",
    "- Contrast with explicit instruction or expert-constructed rules\n",
    "\n",
    "### A2. Data\n",
    "\n",
    "#### A2.1. Labeled and unlabeled\n",
    "\n",
    "- Labeled $\\to$ supervised learning\n",
    "- Unlabeled $\\to$ unsupervised learning (or possibly self-supervised learning)\n",
    "- Future reward $\\to$ reinforcement learning\n",
    "\n",
    "#### A2.2. Structured and unstructured\n",
    "\n",
    "- Structured $\\to$ tabular\n",
    "- Unstructured just means non-tabular - includes free text, image, video, audio, sequences\n",
    "- In the past, unstructured data was first converted to structured data by *feature engineering*; this  has been upended by deep learning methods\n",
    "\n",
    "#### A2.3. Size\n",
    "\n",
    "- Number of observations\n",
    "- Number of features (dimensionality)\n",
    "\n",
    "### A3. ML model examples\n",
    "\n",
    "#### A3.1 Dimension reduction\n",
    "\n",
    "- PCA\n",
    "- MDS\n",
    "- t-SNE\n",
    "- UMAP\n",
    "- PHATE\n",
    "\n",
    "#### A3.2 Clustering\n",
    "\n",
    "- K-means\n",
    "- Agglomerative hierarchical clustering\n",
    "- Mixture models\n",
    "\n",
    "#### A3.3 Supervised learning\n",
    "- Nearest neighbor\n",
    "![img](https://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1531424125/KNN_final_a1mrv9.png)\n",
    "- Linear models\n",
    "![img](https://static.javatpoint.com/tutorial/machine-learning/images/machine-learning-polynomial-regression.png)\n",
    "- Support vector machines\n",
    "![img](https://upload.wikimedia.org/wikipedia/commons/thumb/7/72/SVM_margin.png/300px-SVM_margin.png)\n",
    "- Trees\n",
    "![img](https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2016/02/Example-Decision-Tree.png)\n",
    "- Neural networks\n",
    "![img](https://ml-cheatsheet.readthedocs.io/en/latest/_images/dynamic_resizing_neural_network_4_obs.png)\n",
    "\n",
    "## B. ML stages\n",
    "\n",
    "### B1. Data processing\n",
    "\n",
    "We typically need to process the data for it to work with a broad class of ML models. For example, many ML algorithms will have problems in fitting or interpretation when there are categorical or free text columns, missing data, large variation in measurement scales or collinear features.\n",
    "\n",
    "Note: To avoid data *leakage*, any preprocessing that has a *fit* stage should estimate parameters on *training* data only.\n",
    "\n",
    "#### B1.1. Category encoding\n",
    "\n",
    "- Encoding without labels\n",
    "- Encoding with labels\n",
    "\n",
    "#### B1.2. Missing data imputation\n",
    "\n",
    "- Types of missing data\n",
    "    - MCAR\n",
    "    - MAR\n",
    "    - MNAR\n",
    "- Simple imputation\n",
    "- Fancy imputation\n",
    "\n",
    "#### B1.3. Feature selection\n",
    "\n",
    "- Uninformative variables\n",
    "- Collinear or multi-collinear variables\n",
    "- Dependent features\n",
    "- Recursive feature elimination\n",
    "- LASSO\n",
    "\n",
    "#### B1.4. Shuffling \n",
    "\n",
    "- Shuffling breaks up any order to the observations\n",
    "\n",
    "#### B1.5 Standardization\n",
    "\n",
    "- Distance-based models may be sensitive to scale\n",
    "- Convert features to have zero mean and unit standard deviation\n",
    "\n",
    "### B2. Model training\n",
    "\n",
    "#### B2.1. Memorization and generalization\n",
    "\n",
    "- The entire point of any form of learning is generalization, not memorization\n",
    "- Model capacity is the amount of information a model can store\n",
    "- If model capacity $\\gg$ data complexity, the model will perform best by just memorizing the data $\\to$ over-fitting\n",
    "- If model capacity $\\ll$ data complexity, the model will not be very good $\\to$ under-fitting\n",
    "- Bias and variance\n",
    "![img](https://miro.medium.com/max/544/1*Y-yJiR0FzMgchPA-Fm5c1Q.jpeg)\n",
    "- Bias-variance trade-off\n",
    "![img](http://scott.fortmann-roe.com/docs/docs/BiasVariance/biasvariance.png)\n",
    "\n",
    "#### B2.1.1. Tracking training and validation measures\n",
    "\n",
    "![img](https://jithinjk.github.io/blog/images/leslie_hyperparams/fig2.PNG)\n",
    "\n",
    "Alternative showing what happens as model complexity grows over time with training for high-capacity model\n",
    "\n",
    "![img](https://drek4537l1klr.cloudfront.net/cai/Figures/08fig06_alt.jpg)\n",
    "\n",
    "#### B2.1.2. Remedies for over-fitting\n",
    "\n",
    "- Collect more data\n",
    "- Synthetic data and data augmentation\n",
    "- Pre-training\n",
    "- Early stopping\n",
    "- L1 and L2 regularization\n",
    "- Model-specific parameters for controlling model complexity\n",
    "- Dropout\n",
    "\n",
    "#### B2.1.3. Data leakage\n",
    "\n",
    "- Data leakage occurs when your model has access to information it would not have when making predictions in practice\n",
    "    - This results in over-optimistic model evaluations\n",
    "\n",
    "#### B2.1.4. In-sample and out-of-sample prediction\n",
    "\n",
    "- Train-test split\n",
    "- Importance of out-of-sample prediction\n",
    "- Do not train your model on data that your deployed model will not have access to!\n",
    "    - Model fitting must be done on training data only - this includes data preprocessing estimators\n",
    "    - Importance of using pipelines\n",
    "\n",
    "#### B2.2 Imbalanced data\n",
    "\n",
    "- Choice of evaluation metrics (e.g. [Kappa](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.cohen_kappa_score.html))\n",
    "- Weighting samples\n",
    "- Majority under-sampling\n",
    "- Minority over-sampling\n",
    "\n",
    "#### B2.3. Hyper-parameter tuning\n",
    "\n",
    "- Role of cross-validation\n",
    "- Grid search\n",
    "- Auto-tuning\n",
    "\n",
    "#### B2.4. Ensemble models\n",
    "\n",
    "- Stacking\n",
    "![img](https://blogs.sas.com/content/subconsciousmusings/files/2017/05/modelstacking.png)\n",
    "- Bagging\n",
    "![img](https://pluralsight2.imgix.net/guides/38f3d18e-81a9-471f-9a1c-3172b5fa3246_5.jpg)\n",
    "- Boosting\n",
    "\n",
    "Classifier\n",
    "![img](https://upload.wikimedia.org/wikipedia/commons/thumb/b/b5/Ensemble_Boosting.svg/2560px-Ensemble_Boosting.svg.png)\n",
    "Regressor\n",
    "![img](https://media.geeksforgeeks.org/wp-content/uploads/20200721214745/gradientboosting.PNG)\n",
    "\n",
    "### B3. Model evaluation\n",
    "\n",
    "#### B3.1. Unsupervised learning metrics\n",
    "\n",
    "##### B3.1.1 Dimension reduction\n",
    "\n",
    "- Interpreting PCA\n",
    "    - Explained variance\n",
    "    - Scree plot\n",
    "    - Loadings\n",
    "    - Biplots\n",
    "    \n",
    "![img](https://www.researchgate.net/publication/318706592/figure/fig1/AS:613956045508639@1523389936663/Biplot-of-the-Principal-Component-Analysis-PCA-on-the-autoscaled-data-showing-the_Q640.jpg)\n",
    "\n",
    "##### B3.1.2. Clustering metrics\n",
    "\n",
    "- Rand index\n",
    "- Within and between cluster variance\n",
    "- Similarity of nearby clusters\n",
    "\n",
    "##### B3.1.2. Information criteria for probabilistic models\n",
    "\n",
    "- Negative log likelihood and deviance\n",
    "- AIC\n",
    "- BIC\n",
    "\n",
    "#### B3.2 Supervised learning metrics\n",
    "\n",
    "##### B3.2.1. Predictions\n",
    "\n",
    "- `predict`, `predict_proba`, and `predict_log_proba`\n",
    "\n",
    "##### B3.2.2. The base model\n",
    "\n",
    "- Dummy model (sanity check)\n",
    "\n",
    "##### B3.2.3 Classification metrics\n",
    "\n",
    "- Confusion matrix and binary scores\n",
    "- ROC curve\n",
    "- PR curve\n",
    "- Cumulative gains curve\n",
    "- Discrimination threshold\n",
    "\n",
    "##### B3.2.4. Regression metrics\n",
    "\n",
    "- Metrics\n",
    "- Residual plot\n",
    "- Prediction error plot\n",
    "\n",
    "### B4. Model interpretation\n",
    "\n",
    "#### B4.1. Coefficients\n",
    "\n",
    "- Generally only available with linear model family\n",
    "\n",
    "#### B4.2. Feature importances\n",
    "\n",
    "- Several ways of calculating - widely available for tree-based methods\n",
    "- No direction\n",
    "\n",
    "#### B4.3. Partial dependence plots\n",
    "\n",
    "- Adds direction to feature importances\n",
    "\n",
    "#### B4.4. Surrogate models\n",
    "\n",
    "- Use an explainable model to simulate a black box model\n",
    "\n",
    "#### B4.5. Shapely\n",
    "\n",
    "- SHAP \n",
    "- For one instance\n",
    "- OVer all instances\n",
    "- Interactions / dependencies\n",
    "- Summary plot\n",
    "\n",
    "### Pipelines\n",
    "\n",
    "- Using `sklearn` mixins to create custom ML classes\n",
    "- Using a pipeline for consistency and deployability"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
