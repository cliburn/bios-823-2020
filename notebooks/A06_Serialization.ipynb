{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving and sharing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many data science applications require an intermediate storage format for transfer of data. The data to be stored may be structurally complex or large. One application is serialization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serialization\n",
    "\n",
    "From [Wikipedia](https://en.wikipedia.org/wiki/Serialization)\n",
    "\n",
    "> In computing, serialization (US spelling) or serialisation (UK spelling) is the process of translating a data structure or object state into a format that can be stored (for example, in a file or memory data buffer) or transmitted (for example, across a computer network) and reconstructed later (possibly in a different computer environment)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "For example, in ML applications, we often need to store details about a machine learning model (including train/test data so that we can compare it with other models. These may then need to be transferred across computers to perform comparative analysis.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We illustrate with an example from `scikit-learn` docs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "X, y = make_classification(random_state=0)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    random_state=0)\n",
    "pipe = Pipeline([('scaler', StandardScaler()), ('svc', SVC())])\n",
    "pipe.fit(X_train, y_train)\n",
    "pipe.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We monkey-patch the pipeline to give it a name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.name = 'my_pipeline_0.0.1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A pipeline has several parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also want to know the data used to train and test the model. Here 2 samples of training data are shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We combine these into a single data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python_model = {\n",
    "    'model': pipe,\n",
    "    'X_train': X_train,\n",
    "    'y_train': y_train,\n",
    "    'X_test': X_test,\n",
    "    'y_test': y_test\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pendulum\n",
    "\n",
    "filename_base = f'{pipe.name}_{pendulum.now()}'\n",
    "filename_base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python native data formats\n",
    "\n",
    "If you only ever use Python and don't need to share your data with anyone else, you can use efficient data structures native to Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that we need to open file in write binary\n",
    "pickle_file = f'{filename_base}.pickle'\n",
    "with open(pickle_file, 'wb') as f:\n",
    "    pickle.dump(python_model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! head -c 200 $pickle_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(pickle_file, 'rb') as f:\n",
    "    m_pickle = pickle.load(f)\n",
    "print(m_pickle.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is super convenient because the model is immediately usable!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_pickle['model'].score(m_pickle['X_test'], m_pickle['y_test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joblib\n",
    "\n",
    "Joblib is more efficient for objects with large arrays. Behind the scenes this uses a library called `dill` that is adds some features to `pickle`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib_file = f'{filename_base}.joblib'\n",
    "joblib.dump(python_model, joblib_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! head -c 200 $joblib_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_joblib = joblib.load(joblib_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_joblib['model'].score(m_joblib['X_test'], m_joblib['y_test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Portable data formats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we generally cannot automatically store Python objects, so we create a generic data structure to store. Serialization using these non-native formats usually takes more work. \n",
    "\n",
    "**Note**. Some Python libraries such as `pyyaml` provide mechanisms for directly storing and recreating objects like `pickle` and `joblib` - not covered in lecture notes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generic_model = {\n",
    "    'name': pipe.name,\n",
    "    'params': pipe.get_params(),\n",
    "    'X_train': X_train,\n",
    "    'y_train': y_train,\n",
    "    'X_test': X_test,\n",
    "    'y_test': y_test\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSV\n",
    "\n",
    "CSV cannot handle non-tabular data structures, so we would have to do something like store 5 different files:\n",
    "\n",
    "- model key, value pairs (one per line)\n",
    "- X\\_train\n",
    "- X\\_test\n",
    "- y\\_train\n",
    "- y\\_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "csv_file = f'{pipe.name}_{pendulum.now()}.csv'\n",
    "with open(csv_file, 'w') as f:\n",
    "    writer = csv.writer(f, delimiter=',', quotechar='\"')\n",
    "    writer.writerow(['name', pipe.name])\n",
    "    for k, v in pipe.get_params().items():\n",
    "        writer.writerow([k, v])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! head -c 200 $csv_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading back using the CSV module solves the commas embedded in qutotes problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(csv_file, 'r') as f:\n",
    "    reader = csv.reader(f, delimiter=',', quotechar='\"')\n",
    "    for i, row in enumerate(reader):\n",
    "        print(row)\n",
    "        if i >= 2:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can write the numpy arrays to CSV in the same way, but it's easier to do so directly in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_filename = f'X_train_{filename_base}'\n",
    "np.savetxt(X_train_filename, X_train, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! head -c 200 $X_train_filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading back into `numpy` is also straightforward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.loadtxt(X_train_filename, delimiter=',').shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JSON\n",
    "\n",
    "JSON is ubiquitous as a data format, and is native to the REST API. Generally, JSON only understands basic data types - string, numbers, ,object (this is like a Python dictionary), array (this is like a Python list), boolean and null - so is inefficient for transferring large binary objects such as `numpy` arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, the `get_params` method returns values that are Python objects suhc as `StandardScaler()` that cannot be directly serialized to JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file = f'{filename_base}.json'\n",
    "\n",
    "with open(json_file, 'w') as f:\n",
    "    try:\n",
    "        json.dump(generic_model, f)\n",
    "    except TypeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to convert to strings first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def serialize(m):\n",
    "    \"\"\"Serialize all objects to their string represntation.\"\"\"\n",
    "    d = {}\n",
    "    for k, v in m.items():\n",
    "        if type(v) is np.ndarray:\n",
    "            d[k] = v.tolist()\n",
    "        else:\n",
    "            d[k] = str(v)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(json_file, 'w') as f:\n",
    "    json.dump(serialize(generic_model), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! head -c 200 $json_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The price is that now, everything is a string and you need to do the reconstruction.\n",
    "\n",
    "See [docs](https://stackabuse.com/scikit-learn-save-and-restore-models/) for how to restore `scikit-learn` models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is simple to restore `numpy` arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(json_file, 'r') as f:\n",
    "    m_json = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_json = np.asarray(m_json['X_test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YAML\n",
    "\n",
    "- YAML Ain't Markup Language\n",
    "- YAML is often used for configuration - for example, in `docker-compose` to specify containers\n",
    "\n",
    "YAML is a superset of JSON, so anything that can be serialized as JSON will work. However YAML is more flexible. See YAML [docs](https://yaml.org/spec/1.2/spec.html) for more information - especially how to use YAML aliases and references."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yaml_file = f'{filename_base}.yaml'\n",
    "\n",
    "with open(yaml_file, 'w') as f:\n",
    "    yaml.safe_dump(serialize(generic_model), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! head -c 200 $yaml_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(yaml_file, 'r') as f:\n",
    "    m_yaml = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_yaml.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XML\n",
    "\n",
    "XML is a recursive data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XML is painful to create manually so I will convert from JSON instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python3 -m pip install --quiet json2xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from json2xml import json2xml\n",
    "from json2xml.utils import readfromjson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xml_file = f'{filename_base}.xml'\n",
    "\n",
    "data = readfromjson(json_file)\n",
    "xml = json2xml.Json2xml(data).to_xml()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(xml_file, 'w') as f:\n",
    "    f.write(xml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! head -c 200 $xml_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = ET.parse(xml_file)\n",
    "root = tree.getroot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in root:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use [XPath](https://www.w3schools.com/xml/xpath_syntax.asp) notation to navigate the XML tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = root.find('.//name')\n",
    "name.tag, name.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(root.findall('.//item'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HDF5\n",
    "\n",
    "HDF5 was designed to store large and heterogeneous data sets. It is ideal if you need to store lots of numerical data with annotation.\n",
    "\n",
    "There are two popular libraries in Python:\n",
    "\n",
    "- [h5py](https://docs.h5py.org/en/stable/)\n",
    "- [pytables](https://www.pytables.org)\n",
    "\n",
    "I find `h5py` to have a friendlier interface, but the implementation supported by `pandas` is `pytables`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h5_file = f'{filename_base}.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(h5_file, 'w') as f:\n",
    "    g = f.create_group(pipe.name)\n",
    "    g.create_dataset(name='X_train', data=python_model['X_train'])\n",
    "    g.create_dataset(name='y_train', data=python_model['y_train'])\n",
    "    g.create_dataset(name='X_test', data=python_model['X_test'])\n",
    "    g.create_dataset(name='y_test', data=python_model['y_test'])\n",
    "    g.attrs['name'] = pipe.name\n",
    "    for k, v in pipe.get_params().items():\n",
    "        g.attrs[k] = str(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! head -c 200 $h5_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(h5_file, 'r') as f:\n",
    "    for k in f:\n",
    "        g = f[k]\n",
    "        print(g)\n",
    "        for attr in g.attrs:\n",
    "            print(attr, g.attrs[attr])\n",
    "        for item in (g):\n",
    "            print(item, g[item])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(h5_file, 'r') as f:\n",
    "    xs = f['my_pipeline_0.0.1/X_train']\n",
    "    print(xs[:2, :5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google Protocol Buffer (protobuf)\n",
    "\n",
    "This is typically used to transmit data for ML prediction, especially for ML deployments on a cloud platform. It is a binary buffer, so much more efficient than JSON for large data sets.\n",
    "\n",
    "From the [official docs](https://developers.google.com/protocol-buffers/docs/pythontutorial), there are 3 steps:\n",
    "\n",
    "- Define message formats in a .proto file.\n",
    "- Use the protocol buffer compiler\n",
    "- Use the Python protocol buffer API to write and read messages\n",
    "\n",
    "This will make more sense when we deploy an ML model, so we'll punt the example till then."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
