{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Concepts\n",
    "\n",
    "### History\n",
    "\n",
    "- Motivation\n",
    "    - Move computing to data, not data to computing\n",
    "    - An SSD transfers data at about 500 MB per sec, or 40 minutes to transfer 1 TB\n",
    "    - A  7200 RPM hard disk drive transfers data at about 200 MB per second, or about 1.5 hours to transfer 1 TB.\n",
    "- Google\n",
    "    - Google Distributed Filesystem (GFS)\n",
    "    - Big Table\n",
    "    - Map-reduce\n",
    "- Yahoo!\n",
    "    - Hadoop Distributed File System (HDFS)\n",
    "    - Yet Anohter Resource Negotiator (YARN)\n",
    "    - MapReduce\n",
    "- Limitations of MapReduce\n",
    "    - Cumbersome API\n",
    "    - Every stage reads from/writes to disk\n",
    "    - No native interactive SQL (HIVE, Impala, Drill)\n",
    "    - No native streaming (Storm)\n",
    "    - No native mahcine learning (Mahout)\n",
    "- Spark\n",
    "    - Simple API\n",
    "    - In-memory storage\n",
    "    - Better fault tolerance\n",
    "    - Can take advantage of embarrassingly parallel computations\n",
    "    - Multi-language support (Scala, Java, Python, R)\n",
    "    - Support multiple workloads\n",
    "    - Spark 1.0 released May 11, 2014\n",
    "    - Spark 2.0 released Nov 14, 2016\n",
    "    - Spark 3.0 released Oct 02, 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources\n",
    "\n",
    "- [Quick Start](http://spark.apache.org/docs/latest/quick-start.html)\n",
    "- [Spark Programming Guide](http://spark.apache.org/docs/latest/programming-guide.html)\n",
    "- [DataFramews, DataSets and SQL](http://spark.apache.org/docs/latest/sql-programming-guide.html)\n",
    "- [MLLib](http://spark.apache.org/docs/latest/mllib-guide.html)\n",
    "- [GraphX](http://spark.apache.org/docs/latest/graphx-programming-guide.html)\n",
    "- [Streaming](http://spark.apache.org/docs/latest/streaming-programming-guide.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed computing\n",
    "\n",
    "With distributed computing, you interact with a network of computers that communicate via message passing as if issuing instructions to a single computer.\n",
    "\n",
    "- Distributed execution concepts\n",
    "    - Spark driver (local)\n",
    "        - Spark session\n",
    "        - Spark shell\n",
    "        - Communicates with Spark Master\n",
    "        - Communicates with Spark workers\n",
    "    - Spark master (cluster)\n",
    "        - Resource management on cluster\n",
    "    - Spark workers (cluster)\n",
    "        - Communicate resources to cluster manger\n",
    "        - Start Spark Executors\n",
    "    - Spark executors (cluster)\n",
    "       - Communicate with driver\n",
    "       - Runs task\n",
    "       - Can run multiple threads in parallel\n",
    "- Execution process\n",
    "    - Driver creates jobs\n",
    "        - Each job is a DAG\n",
    "        - DAGScheduler translates into physical plan using RDDs\n",
    "        - Optimization includes merging and splitting into stages\n",
    "        - TaskScheduler distributes physical plans to Executors\n",
    "    - Job consists of one or more stages\n",
    "        - Stage normally ends when there is a need to exchange data (shuffle)\n",
    "    - Stage consists of tasks\n",
    "        - A task is a unit of execution\n",
    "        - Each task is sent to one executor and assigned one data partition\n",
    "        - A multi-core computer can run several tasks in parallel\n",
    "\n",
    "![Distributed computing](https://image.slidesharecdn.com/distributedcomputingwithspark-150414042905-conversion-gate01/95/distributed-computing-with-spark-21-638.jpg?)\n",
    "\n",
    "Source: https://image.slidesharecdn.com/distributedcomputingwithspark-150414042905-conversion-gate01/95/distributed-computing-with-spark-21-638.jpg\n",
    "\n",
    "### Hadoop and Spark\n",
    "\n",
    "- There are 3 major components to a distributed system\n",
    "    - storage\n",
    "    - cluster management\n",
    "    - computing engine\n",
    "\n",
    "- Hadoop is a framework that provides all 3 \n",
    "    - distributed storage (HDFS) \n",
    "    - cluster management (YARN)\n",
    "    - computing engine (MapReduce)\n",
    "    \n",
    "- Spakr only provides the (in-memory) distributed computing engine, and relies on other frameworks for storage and cluster management. It is most frequently used on top of the Hadoop framework, but can also use other distributed storage(e.g. S3 and Cassandra) or cluster management (e.g. Mesos) software.\n",
    "\n",
    "### Distributed storage\n",
    "\n",
    "![storage](http://slideplayer.com/slide/3406872/12/images/15/HDFS+Framework+Key+features+of+HDFS:.jpg)\n",
    "\n",
    "Source: http://slideplayer.com/slide/3406872/12/images/15/HDFS+Framework+Key+features+of+HDFS:.jpg\n",
    "\n",
    "### Role of YARN\n",
    "\n",
    "- Resource manager (manages cluster resources)\n",
    "    - Scheduler\n",
    "    - Applications manager\n",
    "- Node manager (manages single machine/node)\n",
    "    - manages data containers/partitions\n",
    "    - monitors resource usage\n",
    "    - reports to resource manager\n",
    "\n",
    "![Yarn](https://kannandreams.files.wordpress.com/2013/11/yarn1.png)\n",
    "\n",
    "Source: https://kannandreams.files.wordpress.com/2013/11/yarn1.png\n",
    "\n",
    "### YARN operations\n",
    "\n",
    "![yarn ops](https://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/yarn_architecture.gif)\n",
    "\n",
    "Source: https://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/yarn_architecture.gif\n",
    "\n",
    "### Hadoop MapReduce versus Spark\n",
    "\n",
    "Spark has several advantages over Hadoop MapReduce\n",
    "\n",
    "- Use of RAM rahter than disk mean faster processing for multi-step operations\n",
    "- Allows interactive applications\n",
    "- Allows real-time applications\n",
    "- More flexible programming API (full range of functional constructs)\n",
    "\n",
    "![Hadoop](https://i0.wp.com/s3.amazonaws.com/acadgildsite/wordpress_images/bigdatadeveloper/10+steps+to+master+apache+spark/hadoop_spark_1.png)\n",
    "\n",
    "Source: https://i0.wp.com/s3.amazonaws.com/acadgildsite/wordpress_images/bigdatadeveloper/10+steps+to+master+apache+spark/hadoop_spark_1.png\n",
    "\n",
    "### Overall Ecosystem\n",
    "\n",
    "![spark](https://cdn-images-1.medium.com/max/1165/1*z0Vm749Pu6mHdlyPsznMRg.png)\n",
    "\n",
    "Source: https://cdn-images-1.medium.com/max/1165/1*z0Vm749Pu6mHdlyPsznMRg.png\n",
    "\n",
    "### Spark Ecosystem\n",
    "\n",
    "- Spark is written in Scala, a functional programming language built on top of the Java Virtual Machine (JVM)\n",
    "- Traditionally, you have to code in Scala to get the best performance from Spark\n",
    "- With Spark DataFrames and vectorized operations (Spark 2.3 onwards) Python is now competitive\n",
    "\n",
    "![eco](https://databricks.com/wp-content/uploads/2018/12/Spark.jpg)\n",
    "\n",
    "Source: https://databricks.com/wp-content/uploads/2018/12/Spark.jpg\n",
    "\n",
    "### Livy and Spark magic\n",
    "\n",
    "- Livy provides a REST interface to a Spark cluster.\n",
    "\n",
    "![Livy](https://cdn-images-1.medium.com/max/956/0*-lwKpnEq0Tpi3Tlj.png)\n",
    "\n",
    "Source: https://cdn-images-1.medium.com/max/956/0*-lwKpnEq0Tpi3Tlj.png\n",
    "\n",
    "### PySpark\n",
    "\n",
    "![PySpark](http://i.imgur.com/YlI8AqEl.png)\n",
    "\n",
    "Source: http://i.imgur.com/YlI8AqEl.png\n",
    "\n",
    "### Resilient distributed datasets (RDDs)\n",
    "\n",
    "![rdd](https://miro.medium.com/max/1152/1*l2MUHFvWfcdiUbh7Y-fM5Q.png)\n",
    "\n",
    "Source: https://miro.medium.com/max/1152/1*l2MUHFvWfcdiUbh7Y-fM5Q.png\n",
    "\n",
    "### Spark fault tolerance\n",
    "\n",
    "![graph](https://image.slidesharecdn.com/deep-dive-with-spark-streamingtathagata-dasspark-meetup2013-06-17-130623151510-phpapp02/95/deep-dive-with-spark-streaming-tathagata-das-spark-meetup-20130617-13-638.jpg)\n",
    "\n",
    "Source: https://image.slidesharecdn.com/deep-dive-with-spark-streamingtathagata-dasspark-meetup2013-06-17-130623151510-phpapp02/95/deep-dive-with-spark-streaming-tathagata-das-spark-meetup-20130617-13-638.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hadoop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Java 11 gives warning messages with `hdfs`. This futility `print_hadooop` unction just removes the clutter of the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_hadoop(s):\n",
    "    for line in s.splitlines():\n",
    "        if 'WARN' in line or 'JAVA_TOOL_OPTIONS' in line:\n",
    "            continue\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HDFS\n",
    "\n",
    "Working with files in HDFS is similar to working with files in a regular Unix shell. Some commonly used commands are illustrated below.\n",
    "\n",
    "The commands below will only work if there is a local installation of HDFS and a local user directory has been created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List contents of HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture out\n",
    "! hdfs dfs -ls -R | head -n4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_hadoop(out.stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture out\n",
    "! hdfs dfs -mkdir csv notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Copy files from HDFS to HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture out\n",
    "! hdfs dfs -cp data/*csv csv/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cp: `data/SacramentocrimeJanuary2006.csv': No such file or directory\n",
      "cp: `data/X_test.csv': No such file or directory\n",
      "cp: `data/X_test_unscaled.csv': No such file or directory\n",
      "cp: `data/X_train.csv': No such file or directory\n",
      "cp: `data/X_train_unscaled.csv': No such file or directory\n",
      "cp: `data/flat.csv': No such file or directory\n",
      "cp: `data/nile.csv': No such file or directory\n",
      "cp: `data/profiles.csv': No such file or directory\n",
      "cp: `data/test.csv': No such file or directory\n",
      "cp: `data/test_null.csv': No such file or directory\n",
      "cp: `data/uk-deaths-from-bronchitis-emphys.csv': No such file or directory\n",
      "cp: `data/y_test.csv': No such file or directory\n",
      "cp: `data/y_test_unscaled.csv': No such file or directory\n",
      "cp: `data/y_train.csv': No such file or directory\n",
      "cp: `data/y_train_unscaled.csv': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "print_hadoop(out.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture out\n",
    "! hdfs dfs -ls csv | head -n4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_hadoop(out.stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Copy from local to HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19A_Stream_Generator.ipynb\r\n",
      "19B_Spark_Streaming.ipynb\r\n",
      "19C_Spark_Streaming.ipynb\r\n",
      "19D_Spark_Streaming.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "! ls | head -n4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture out\n",
    "! hdfs dfs -copyFromLocal A*ipynb notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_hadoop(out.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture out\n",
    "! hdfs dfs -ls notebooks| head -n4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 16 items\n",
      "-rw-r--r--   1 cliburnchan supergroup      25709 2020-11-02 13:57 notebooks/A01_Python_Concepts.ipynb\n",
      "-rw-r--r--   1 cliburnchan supergroup      25709 2020-11-02 13:57 notebooks/A01_copied.ipynb\n",
      "-rw-r--r--   1 cliburnchan supergroup      11433 2020-11-02 13:57 notebooks/A02_Numpy_Concepts.ipynb\n"
     ]
    }
   ],
   "source": [
    "print_hadoop(out.stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Copy from HDFS to local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture out\n",
    "! hdfs dfs -copyToLocal notebooks/A01_Python_Concepts.ipynb A01_copied.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A01_Python_Concepts.ipynb\r\n",
      "A01_copied.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "! ls -1 A01*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture out\n",
    "! hdfs -version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "java version \"11.0.9\" 2020-10-20 LTS\n",
      "Java(TM) SE Runtime Environment 18.9 (build 11.0.9+7-LTS)\n",
      "Java HotSpot(TM) 64-Bit Server VM 18.9 (build 11.0.9+7-LTS, mixed mode)\n"
     ]
    }
   ],
   "source": [
    "print_hadoop(out.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture out\n",
    "! hdfs dfs -du -h ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        csv\n",
      "358.2 K  notebooks\n"
     ]
    }
   ],
   "source": [
    "print_hadoop(out.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture out\n",
    "! hdfs dfs -df -h ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filesystem                Size     Used  Available  Use%\n",
      "hdfs://localhost:9000  931.5 G  365.1 K    347.6 G    0%\n"
     ]
    }
   ],
   "source": [
    "print_hadoop(out.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture out\n",
    "! hdfs dfs -help count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-count [-q] [-h] <path> ... :\n",
      "  Count the number of directories, files and bytes under the paths\n",
      "  that match the specified file pattern.  The output columns are:\n",
      "  DIR_COUNT FILE_COUNT CONTENT_SIZE FILE_NAME or\n",
      "  QUOTA REMAINING_QUOTA SPACE_QUOTA REMAINING_SPACE_QUOTA \n",
      "        DIR_COUNT FILE_COUNT CONTENT_SIZE FILE_NAME\n",
      "  The -h option shows file sizes in human readable format.\n"
     ]
    }
   ],
   "source": [
    "print_hadoop(out.stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MapReduce\n",
    "\n",
    "If you are interested in MapReduce, see the official [tutorial](https://hadoop.apache.org/docs/current/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html#Example:_WordCount_v1.0)\n",
    "\n",
    "We will jump straight to Spark, which has a much nicer API for data scientists and statisticians."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Spark\n",
    "\n",
    "- If necessary, install [Java](https://java.com/en/download/help/download_options.xml)\n",
    "- Downlaod and install [Sppark](http://spark.apache.org/downloads.html)\n",
    "```bash\n",
    "wget https://apache.claz.org/spark/spark-3.0.1/spark-3.0.1-bin-hadoop2.7.tgz\n",
    "\n",
    "tar xzf spark-3.0.3-bin-hadoop2.7.tgz\n",
    "sudo mv spark-3.0.1-bin-hadoop2.7 /usr/local/spark\n",
    "```\n",
    "Set up graphframes\n",
    "```bash\n",
    "python3 -m pip install graphframes\n",
    "```\n",
    "Set up environment variables\n",
    "```bash\n",
    "export PATH=$PATH:/usr/local/spark/bin\n",
    "export SPARK_HOME=/usr/local/spark\n",
    "export PYTHONPATH=$SPARK_HOME/python:$PYTHONPATH\n",
    "export PYSPARK_DRIVER_PYTHON=\"jupyter\"\n",
    "export PYSPARK_DRIVER_PYTHON_OPTS=\"notebook\"\n",
    "export PYSPARK_PYTHON=python3\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  `sparkmagic` (Optional)\n",
    "\n",
    "Install and start `livy`\n",
    "```\n",
    "cd ~\n",
    "wget https://www.apache.org/dyn/closer.lua/incubator/livy/0.7.0-incubating/apache-livy-0.7.0-incubating-bin.zip\n",
    "unzip apache-livy-0.7.0-incubating-bin.zip\n",
    "mv apache-livy-0.7.0-incubating-bin livy\n",
    "livy/bin/livy-server start\n",
    "```\n",
    "\n",
    "Install `sparkmagic`\n",
    "\n",
    "```\n",
    "python3 -m pip install sparkmagic\n",
    "jupyter nbextension enable --py --sys-prefix widgetsnbextension \n",
    "```\n",
    "\n",
    "Type `pip show sparkmagic` and cd to the directory shown in LOCATION\n",
    "\n",
    "```\n",
    "jupyter-kernelspec install sparkmagic/kernels/pysparkkernel\n",
    "jupyter serverextension enable --py sparkmagic\n",
    "```\n",
    "\n",
    "For the adventurous, see [Running Spark on an AWS EMR cluster](https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-spark.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark UI\n",
    "\n",
    "- Default port 4040 http://localhost:4040/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file candy.csv\n",
    "name,age,candy\n",
    "tom,3,m&m\n",
    "shirley,6,mentos\n",
    "david,4,candy floss\n",
    "anne,5,starburst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('csv/SacramentocrimeJanuary2006.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
