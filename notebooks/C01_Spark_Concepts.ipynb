{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Concepts\n",
    "\n",
    "### History\n",
    "\n",
    "- Motivation\n",
    "    - Move computing to data, not data to computing\n",
    "- Google\n",
    "    Google Distributed Filesystem (GFS)\n",
    "    Big Table\n",
    "    Map-reduce\n",
    "- Yahoo!\n",
    "    - Hadoop Distributed File System (HDFS)\n",
    "    - Yet Anohter Resource Negotiator (YARN)\n",
    "    - MapReduce\n",
    "- Limitations of MapReduce\n",
    "    - Cumbersome API\n",
    "    - Every stage reads from/writes to disk\n",
    "    - No native interactive SQL (HIVE, Impala, Drill)\n",
    "    - No native streaming (Storm)\n",
    "    - No native mahcine learning (Mahout)\n",
    "- Spark\n",
    "    - Simple API\n",
    "    - In-memory storage\n",
    "    - Better fault tolerance\n",
    "    - Can take advantage of embarrassingly parallel computations\n",
    "    - Multi-language support (Scala, Java, Python, R)\n",
    "    - Support multiple workloads\n",
    "    - Spark 1.0 released May 11, 2014\n",
    "    - Spark 2.0 released Nov 14, 2016\n",
    "    - Spark 3.0 released Oct 02, 2020\n",
    "- Speed\n",
    "    - DAG scheduler\n",
    "    - Catalyst query optimizer\n",
    "    - Tungsten execution engine\n",
    "- Ease of use\n",
    "    - Resilient Distributed Dataset (RDD)\n",
    "    - Lazy evaluation\n",
    "- Modularity\n",
    "    - Languages\n",
    "    - APIs - Strucrtured Srtreaming, GraphX, MLLib\n",
    "- Extensible\n",
    "    - Data readers\n",
    "    - Data writers\n",
    "    - [3rd party connectors](https://spark.apache.org/third-party-projects.html)\n",
    "    - Multiple platforms - local, data center, cloud, kubernetes\n",
    "- Distributed execution concepts\n",
    "    - Spark driver\n",
    "        - Spark session\n",
    "        - Spark shell\n",
    "        - Communicates with Spark Master\n",
    "        - Communicates with Spakr workers\n",
    "    - Spark master\n",
    "        - Resource management on cluster\n",
    "    - Spark workers\n",
    "        - Communicate reosuces to cluster manger\n",
    "        - Start Spark Executors\n",
    "    - Spark executors\n",
    "       - Communicate with driver\n",
    "       - Runs task\n",
    "       - Can run multiple threds in parallel\n",
    "- Execution process\n",
    "    - Driver creates jobs\n",
    "        - Each job is a DAG\n",
    "        - DAGScheduler translates into physical plan using RDDs\n",
    "        - Optimization incldues merging and splitiing into stages\n",
    "        - TaskScheduler distributes physical plans to Executors\n",
    "    - Job consists of one or more stages\n",
    "        - Stage normally ends when there is a need to exchange data (shuffle)\n",
    "    - Stage consists of tasks\n",
    "        - A task is a unit of execution\n",
    "        - Each task is sent to one executor and assigned one data partition\n",
    "        - A mutli-core computer can run several tasks in parallel\n",
    "- Transforms and actions\n",
    "    - Transforms are lazy\n",
    "    - Actions are eager\n",
    "    - NArrow versus broad transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources\n",
    "\n",
    "- [Quick Start](http://spark.apache.org/docs/latest/quick-start.html)\n",
    "- [Spark Programming Guide](http://spark.apache.org/docs/latest/programming-guide.html)\n",
    "- [DataFramews, DataSets and SQL](http://spark.apache.org/docs/latest/sql-programming-guide.html)\n",
    "- [MLLib](http://spark.apache.org/docs/latest/mllib-guide.html)\n",
    "- [GraphX](http://spark.apache.org/docs/latest/graphx-programming-guide.html)\n",
    "- [Streaming](http://spark.apache.org/docs/latest/streaming-programming-guide.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed computing bakkground\n",
    "\n",
    "With distributed computing, you interact with a network of computers that communicate via message passing as if issuing instructions to a single computer.\n",
    "\n",
    "![Distributed computing](https://image.slidesharecdn.com/distributedcomputingwithspark-150414042905-conversion-gate01/95/distributed-computing-with-spark-21-638.jpg?)\n",
    "\n",
    "Source: https://image.slidesharecdn.com/distributedcomputingwithspark-150414042905-conversion-gate01/95/distributed-computing-with-spark-21-638.jpg\n",
    "\n",
    "### Hadoop and Spark\n",
    "\n",
    "- There are 3 major components to a distributed system\n",
    "    - storage\n",
    "    - cluster management\n",
    "    - computing engine\n",
    "\n",
    "- Hadoop is a framework that provides all 3 \n",
    "    - distributed storage (HDFS) \n",
    "    - clsuter managemnet (YARN)\n",
    "    - computing eneine (MapReduce)\n",
    "    \n",
    "- Spakr only provides the (in-memory) distributed computing engine, and relies on other frameworks for storage and cluster management. It is most frequently used on top of the Hadoop framework, but can also use other distributed storage(e.g. S3 and Cassandra) or cluster mangement (e.g. Mesos) software.\n",
    "\n",
    "### Distributed storage\n",
    "\n",
    "![storage](http://slideplayer.com/slide/3406872/12/images/15/HDFS+Framework+Key+features+of+HDFS:.jpg)\n",
    "\n",
    "Source: http://slideplayer.com/slide/3406872/12/images/15/HDFS+Framework+Key+features+of+HDFS:.jpg\n",
    "\n",
    "### Role of YARN\n",
    "\n",
    "- Resource manager (manages cluster resources)\n",
    "    - Scheduler\n",
    "    - Applications manager\n",
    "- Node manager (manages single machine/node)\n",
    "    - manages data containers/partitions\n",
    "    - monitors resource usage\n",
    "    - reports to resource manager\n",
    "\n",
    "![Yarn](https://kannandreams.files.wordpress.com/2013/11/yarn1.png)\n",
    "\n",
    "Source: https://kannandreams.files.wordpress.com/2013/11/yarn1.png\n",
    "\n",
    "### YARN operations\n",
    "\n",
    "![yarn ops](https://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/yarn_architecture.gif)\n",
    "\n",
    "Source: https://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/yarn_architecture.gif\n",
    "\n",
    "### Hadoop MapReduce versus Spark\n",
    "\n",
    "Spark has several advantages over Hadoop MapReduce\n",
    "\n",
    "- Use of RAM rahter than disk mean faster processing for multi-step operations\n",
    "- Allows interactive applications\n",
    "- Allows real-time applications\n",
    "- More flexible programming API (full range of functional constructs)\n",
    "\n",
    "![Hadoop](https://i0.wp.com/s3.amazonaws.com/acadgildsite/wordpress_images/bigdatadeveloper/10+steps+to+master+apache+spark/hadoop_spark_1.png)\n",
    "\n",
    "Source: https://i0.wp.com/s3.amazonaws.com/acadgildsite/wordpress_images/bigdatadeveloper/10+steps+to+master+apache+spark/hadoop_spark_1.png\n",
    "\n",
    "### Overall Ecosystem\n",
    "\n",
    "![spark](https://cdn-images-1.medium.com/max/1165/1*z0Vm749Pu6mHdlyPsznMRg.png)\n",
    "\n",
    "Source: https://cdn-images-1.medium.com/max/1165/1*z0Vm749Pu6mHdlyPsznMRg.png\n",
    "\n",
    "### Spark Ecosystem\n",
    "\n",
    "- Spark is written in Scala, a functional programming language built on top of the Java Virtual Machine (JVM)\n",
    "- Traditionally, you have to code in Scala to get the best performance from Spark\n",
    "- With Spark DataFrames and vectorized operations (Spark 2.3 onwards) Python is now competitive\n",
    "\n",
    "![eco](https://databricks.com/wp-content/uploads/2018/12/Spark.jpg)\n",
    "\n",
    "Source: https://databricks.com/wp-content/uploads/2018/12/Spark.jpg\n",
    "\n",
    "### Livy and Spark magic\n",
    "\n",
    "- Livy provides a REST interface to a Spark cluster.\n",
    "\n",
    "![Livy](https://cdn-images-1.medium.com/max/956/0*-lwKpnEq0Tpi3Tlj.png)\n",
    "\n",
    "Source: https://cdn-images-1.medium.com/max/956/0*-lwKpnEq0Tpi3Tlj.png\n",
    "\n",
    "### PySpark\n",
    "\n",
    "![PySpark](http://i.imgur.com/YlI8AqEl.png)\n",
    "\n",
    "Source: http://i.imgur.com/YlI8AqEl.png\n",
    "\n",
    "### Resilient distributed datasets (RDDs)\n",
    "\n",
    "![rdd](https://mapr.com/blog/real-time-streaming-data-pipelines-apache-apis-kafka-spark-streaming-and-hbase/assets/blogimages/msspark/imag12.png)\n",
    "\n",
    "Source: https://mapr.com/blog/real-time-streaming-data-pipelines-apache-apis-kafka-spark-streaming-and-hbase/assets/blogimages/msspark/imag12.png\n",
    "\n",
    "### Spark fault tolerance\n",
    "\n",
    "![graph](https://image.slidesharecdn.com/deep-dive-with-spark-streamingtathagata-dasspark-meetup2013-06-17-130623151510-phpapp02/95/deep-dive-with-spark-streaming-tathagata-das-spark-meetup-20130617-13-638.jpg)\n",
    "\n",
    "Source: https://image.slidesharecdn.com/deep-dive-with-spark-streamingtathagata-dasspark-meetup2013-06-17-130623151510-phpapp02/95/deep-dive-with-spark-streaming-tathagata-das-spark-meetup-20130617-13-638.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_hadoop(s):\n",
    "    for line in s.splitlines():\n",
    "        if 'WARN' in line or 'JAVA_TOOL_OPTIONS' in line:\n",
    "            continue\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HDFS\n",
    "\n",
    "Working with files in HDFS is similar to working with files in a regular Unix shell. Some commonly used commands are illustrated below.\n",
    "\n",
    "The commands below will only work if there is a local installation of HDFS and a local user directory has been created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List contents of HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture out\n",
    "! hdfs dfs -ls -R | head -n4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--   1 cliburnchan supergroup        883 2020-10-23 14:47 core-site.xml\n",
      "drwxr-xr-x   - cliburnchan supergroup          0 2020-10-26 14:54 csv\n",
      "-rw-r--r--   1 cliburnchan supergroup     793574 2020-10-26 14:54 csv/SacramentocrimeJanuary2006.csv\n",
      "-rw-r--r--   1 cliburnchan supergroup      72366 2020-10-26 14:54 csv/X_test.csv\n"
     ]
    }
   ],
   "source": [
    "print_hadoop(out.stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture out\n",
    "! hdfs dfs -mkdir csv notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Copy files from HDFS to HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture out\n",
    "! hdfs dfs -cp data/*csv csv/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cp: `csv/SacramentocrimeJanuary2006.csv': File exists\n",
      "cp: `csv/X_test.csv': File exists\n",
      "cp: `csv/X_test_unscaled.csv': File exists\n",
      "cp: `csv/X_train.csv': File exists\n",
      "cp: `csv/X_train_unscaled.csv': File exists\n",
      "cp: `csv/flat.csv': File exists\n",
      "cp: `csv/nile.csv': File exists\n",
      "cp: `csv/profiles.csv': File exists\n",
      "cp: `csv/test.csv': File exists\n",
      "cp: `csv/test_null.csv': File exists\n",
      "cp: `csv/uk-deaths-from-bronchitis-emphys.csv': File exists\n",
      "cp: `csv/y_test.csv': File exists\n",
      "cp: `csv/y_test_unscaled.csv': File exists\n",
      "cp: `csv/y_train.csv': File exists\n",
      "cp: `csv/y_train_unscaled.csv': File exists\n"
     ]
    }
   ],
   "source": [
    "print_hadoop(out.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture out\n",
    "! hdfs dfs -ls csv | head -n4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 15 items\n",
      "-rw-r--r--   1 cliburnchan supergroup     793574 2020-10-26 14:54 csv/SacramentocrimeJanuary2006.csv\n",
      "-rw-r--r--   1 cliburnchan supergroup      72366 2020-10-26 14:54 csv/X_test.csv\n",
      "-rw-r--r--   1 cliburnchan supergroup      15169 2020-10-26 14:54 csv/X_test_unscaled.csv\n"
     ]
    }
   ],
   "source": [
    "print_hadoop(out.stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Copy from local to HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-material\r\n",
      "A01_Python_Concepts.ipynb\r\n",
      "A01_copied.ipynb\r\n",
      "A02_Numpy_Concepts.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "! ls | head -n4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture out\n",
    "! hdfs dfs -copyFromLocal A*ipynb notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "copyFromLocal: `notebooks/A01_Python_Concepts.ipynb': File exists\n",
      "copyFromLocal: `notebooks/A02_Numpy_Concepts.ipynb': File exists\n",
      "copyFromLocal: `notebooks/A03_Data_Processing.ipynb': File exists\n",
      "copyFromLocal: `notebooks/A04_Visualization.ipynb': File exists\n",
      "copyFromLocal: `notebooks/A04_Visualization_Basics.ipynb': File exists\n",
      "copyFromLocal: `notebooks/A05_File_Formats.ipynb': File exists\n",
      "copyFromLocal: `notebooks/A06_Serialization.ipynb': File exists\n",
      "copyFromLocal: `notebooks/A07A_Normalization_concepts.ipynb': File exists\n",
      "copyFromLocal: `notebooks/A07_Relatinoal_Databases.ipynb': File exists\n",
      "copyFromLocal: `notebooks/A08_SQL_Queries_01.ipynb': File exists\n",
      "copyFromLocal: `notebooks/A09_SQL_Queries_02.ipynb': File exists\n",
      "copyFromLocal: `notebooks/A10_NoSQL_MongoDB.ipynb': File exists\n",
      "copyFromLocal: `notebooks/A11_NoSQL_Redis.ipynb': File exists\n",
      "copyFromLocal: `notebooks/A12A_Graph_Algorithms.ipynb': File exists\n",
      "copyFromLocal: `notebooks/A12_NoSQL_Neo4j.ipynb': File exists\n"
     ]
    }
   ],
   "source": [
    "print_hadoop(out.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture out\n",
    "! hdfs dfs -ls notebooks| head -n4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 16 items\n",
      "-rw-r--r--   1 cliburnchan supergroup      25709 2020-10-26 15:00 notebooks/A01_Python_Concepts.ipynb\n",
      "-rw-r--r--   1 cliburnchan supergroup      25709 2020-10-26 15:33 notebooks/A01_copied.ipynb\n",
      "-rw-r--r--   1 cliburnchan supergroup      11433 2020-10-26 15:00 notebooks/A02_Numpy_Concepts.ipynb\n"
     ]
    }
   ],
   "source": [
    "print_hadoop(out.stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Copy from HDFS to local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture out\n",
    "! hdfs dfs -copyToLocal notebooks/A01_Python_Concepts.ipynb A01_copied.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A01_Python_Concepts.ipynb\r\n",
      "A01_copied.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "! ls -1 A01*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture out\n",
    "! hdfs -version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "java version \"11.0.9\" 2020-10-20 LTS\n",
      "Java(TM) SE Runtime Environment 18.9 (build 11.0.9+7-LTS)\n",
      "Java HotSpot(TM) 64-Bit Server VM 18.9 (build 11.0.9+7-LTS, mixed mode)\n"
     ]
    }
   ],
   "source": [
    "print_hadoop(out.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture out\n",
    "! hdfs dfs -du -h ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "883      core-site.xml\n",
      "1.2 M    csv\n",
      "9.7 M    data\n",
      "0        figs\n",
      "358.2 K  notebooks\n"
     ]
    }
   ],
   "source": [
    "print_hadoop(out.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture out\n",
    "! hdfs dfs -df -h ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filesystem                Size    Used  Available  Use%\n",
      "hdfs://localhost:9000  931.5 G  12.3 M    345.5 G    0%\n"
     ]
    }
   ],
   "source": [
    "print_hadoop(out.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture out\n",
    "! hdfs dfs -help count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-count [-q] [-h] <path> ... :\n",
      "  Count the number of directories, files and bytes under the paths\n",
      "  that match the specified file pattern.  The output columns are:\n",
      "  DIR_COUNT FILE_COUNT CONTENT_SIZE FILE_NAME or\n",
      "  QUOTA REMAINING_QUOTA SPACE_QUOTA REMAINING_SPACE_QUOTA \n",
      "        DIR_COUNT FILE_COUNT CONTENT_SIZE FILE_NAME\n",
      "  The -h option shows file sizes in human readable format.\n"
     ]
    }
   ],
   "source": [
    "print_hadoop(out.stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MapReduce\n",
    "\n",
    "If you are interested in MapReduce, see the official [tutorial](https://hadoop.apache.org/docs/current/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html#Example:_WordCount_v1.0)\n",
    "\n",
    "We will jump straight to Spark, which has a much nicer API for data scientists and statisticians."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Spark\n",
    "\n",
    "- If necessary, install [Java](https://java.com/en/download/help/download_options.xml)\n",
    "- Downlaod and install [Sppark](http://spark.apache.org/downloads.html)\n",
    "```bash\n",
    "wget https://apache.claz.org/spark/spark-3.0.1/spark-3.0.1-bin-hadoop2.7.tgz\n",
    "\n",
    "tar xzf spark-3.0.3-bin-hadoop2.7.tgz\n",
    "sudo mv spark-3.0.1-bin-hadoop2.7 /usr/local/spark\n",
    "```\n",
    "Set up graphframes\n",
    "```bash\n",
    "python3 -m pip install graphframes\n",
    "```\n",
    "Set up environment variables\n",
    "```bash\n",
    "export PATH=$PATH:/usr/local/spark/bin\n",
    "export SPARK_HOME=/usr/local/spark\n",
    "export PYTHONPATH=$SPARK_HOME/python:$PYTHONPATH\n",
    "export PYSPARK_DRIVER_PYTHON=\"jupyter\"\n",
    "export PYSPARK_DRIVER_PYTHON_OPTS=\"notebook\"\n",
    "export PYSPARK_PYTHON=python3\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  `sparkmagic` (Optional)\n",
    "\n",
    "Install and start `livy`\n",
    "```\n",
    "cd ~\n",
    "wget https://www.apache.org/dyn/closer.lua/incubator/livy/0.7.0-incubating/apache-livy-0.7.0-incubating-bin.zip\n",
    "unzip apache-livy-0.7.0-incubating-bin.zip\n",
    "mv apache-livy-0.7.0-incubating-bin livy\n",
    "livy/bin/livy-server start\n",
    "```\n",
    "\n",
    "Install `sparkmagic`\n",
    "\n",
    "```\n",
    "python3 -m pip install sparkmagic\n",
    "jupyter nbextension enable --py --sys-prefix widgetsnbextension \n",
    "```\n",
    "\n",
    "Type `pip show sparkmagic` and cd to the directory shown in LOCATION\n",
    "\n",
    "```\n",
    "jupyter-kernelspec install sparkmagic/kernels/pysparkkernel\n",
    "jupyter serverextension enable --py sparkmagic\n",
    "```\n",
    "\n",
    "For the adventurous, see [Running Spark on an AWS EMR cluster](https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-spark.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark UI\n",
    "\n",
    "- Default port 4040 http://localhost:4040/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting candy.csv\n"
     ]
    }
   ],
   "source": [
    "%%file candy.csv\n",
    "name,age,candy\n",
    "tom,3,m&m\n",
    "shirley,6,mentos\n",
    "david,4,candy floss\n",
    "anne,5,starburst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('csv/SacramentocrimeJanuary2006.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------------+--------+----------+----+--------------------+-------------+-----------+------------+\n",
      "|        _c0|                _c1|     _c2|       _c3| _c4|                 _c5|          _c6|        _c7|         _c8|\n",
      "+-----------+-------------------+--------+----------+----+--------------------+-------------+-----------+------------+\n",
      "|  cdatetime|            address|district|      beat|grid|          crimedescr|ucr_ncic_code|   latitude|   longitude|\n",
      "|1/1/06 0:00| 3108 OCCIDENTAL DR|       3|3C        |1115|10851(A)VC TAKE V...|         2404|38.55042047|-121.3914158|\n",
      "|1/1/06 0:00|2082 EXPEDITION WAY|       5|5A        |1512|459 PC  BURGLARY ...|         2204|38.47350069|-121.4901858|\n",
      "|1/1/06 0:00|         4 PALEN CT|       2|2A        | 212|10851(A)VC TAKE V...|         2404|38.65784584|-121.4621009|\n",
      "|1/1/06 0:00|     22 BECKFORD CT|       6|6C        |1443|476 PC PASS FICTI...|         2501|38.50677377|-121.4269508|\n",
      "|1/1/06 0:00|   3421 AUBURN BLVD|       2|2A        | 508|459 PC  BURGLARY-...|         2299| 38.6374478|-121.3846125|\n",
      "|1/1/06 0:00| 5301 BONNIEMAE WAY|       6|6B        |1084|530.5 PC USE PERS...|         2604|38.52697863|-121.4513383|\n",
      "|1/1/06 0:00|      2217 16TH AVE|       4|4A        | 957|459 PC  BURGLARY ...|         2299|  38.537173|-121.4875774|\n",
      "|1/1/06 0:00|          3547 P ST|       3|3C        | 853|484 PC   PETTY TH...|         2308|38.56433456|-121.4618826|\n",
      "|1/1/06 0:00|   3421 AUBURN BLVD|       2|2A        | 508|459 PC  BURGLARY ...|         2203| 38.6374478|-121.3846125|\n",
      "+-----------+-------------------+--------+----------+----+--------------------+-------------+-----------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
