{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_mutual_info_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('dark_background')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toy example to illustrate concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npts = 1000\n",
    "nc = 6\n",
    "X, y = make_blobs(n_samples=npts, centers=nc, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0], X[:, 1], s=5, c=y,\n",
    "            cmap=plt.cm.get_cmap('Accent', nc))\n",
    "plt.axis('square')\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sometimes the structure is clearer with contours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(X[:, 0], X[:, 1])\n",
    "plt.axis('square');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to cluster\n",
    "\n",
    "1. Choose a clustering algorithm\n",
    "2. Choose the number of clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using a library function\n",
    "\n",
    "Normally, we would use a library function. The examples below are meant to illustrate how the k-means algorithm works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(nc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = kmeans.fit_predict(X)\n",
    "centers = kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0], X[:, 1], s=5, c=z,\n",
    "            cmap=plt.cm.get_cmap('Accent', nc))\n",
    "plt.scatter(centers[:, 0], centers[:, 1], marker='x', \n",
    "            linewidth=3, s=100, c='red')\n",
    "plt.axis('square')\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Coding from the ground up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many different clustering methods, but K-means is fast, scales well, and can be interpreted as a probabilistic model. We will write 3 versions of the K-means algorithm to illustrate the main concepts. The algorithm is very simple:\n",
    "\n",
    "1. Start with $k$ centers with labels $0, 1, \\ldots, k-1$\n",
    "2. Find the distance of each data point to each center\n",
    "3. Assign the data points nearest to a center to its label\n",
    "4. Use the mean of the points assigned to a center as the new center\n",
    "5. Repeat for a fixed number of iterations or until the centers stop changing\n",
    "\n",
    "Note: If you want a probabilistic interpretation, just treat the final solution as a mixture of (multivariate) normal distributions. K-means is often used to initialize the fitting of full statistical mixture models, which are computationally more demanding and hence slower. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distance between sets of vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pts = np.arange(6).reshape(-1,2)\n",
    "pts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mus = np.arange(4).reshape(-1, 2)\n",
    "mus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdist(pts, mus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Version 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_kemans_1(X, k, iters=10):\n",
    "    \"\"\"K-means with fixed number of iterations.\"\"\"\n",
    "    \n",
    "    r, c = X.shape\n",
    "    centers = X[np.random.choice(r, k, replace=False)]\n",
    "    for i in range(iters):\n",
    "        m = cdist(X, centers)\n",
    "        z = np.argmin(m, axis=1)\n",
    "        centers = np.array([np.mean(X[z==i], axis=0) for i in range(k)])\n",
    "    return (z, centers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2018)\n",
    "z, centers = my_kemans_1(X, nc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note that K-means can get stuck in local optimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0], X[:, 1], s=5, c=z,\n",
    "            cmap=plt.cm.get_cmap('Accent', nc))\n",
    "plt.scatter(centers[:, 0], centers[:, 1], marker='x', \n",
    "            linewidth=3, s=100, c='red')\n",
    "plt.axis('square')\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Version 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_kemans_2(X, k, tol= 1e-6):\n",
    "    \"\"\"K-means with tolerance.\"\"\"\n",
    "    \n",
    "    r, c = X.shape\n",
    "    centers = X[np.random.choice(r, k, replace=False)]\n",
    "    delta = np.infty\n",
    "    while delta > tol:\n",
    "        m = cdist(X, centers)\n",
    "        z = np.argmin(m, axis=1)\n",
    "        new_centers = np.array([np.mean(X[z==i], axis=0) for i in range(k)])\n",
    "        delta = np.sum((new_centers - centers)**2)\n",
    "        centers = new_centers\n",
    "    return (z, centers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2018)\n",
    "z, centers = my_kemans_2(X, nc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Still stuck in local optimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0], X[:, 1], s=5, c=z,\n",
    "            cmap=plt.cm.get_cmap('Accent', nc))\n",
    "plt.scatter(centers[:, 0], centers[:, 1], marker='x', \n",
    "            linewidth=3, s=100, c='red')\n",
    "plt.axis('square')\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Version 3\n",
    "\n",
    "Use of a score to evaluate the goodness of fit. In this case, the simplest score is just the sum of distances from each point to its nearest center."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_kemans_3(X, k, tol=1e-6, n_starts=10):\n",
    "    \"\"\"K-means with tolerance and random restarts.\"\"\"\n",
    "    \n",
    "    r, c = X.shape\n",
    "    best_score = np.infty\n",
    "  \n",
    "    for i in range(n_starts):\n",
    "        centers = X[np.random.choice(r, k, replace=False)]\n",
    "        delta = np.infty\n",
    "        while delta > tol:\n",
    "            m = cdist(X, centers)\n",
    "            z = np.argmin(m, axis=1)\n",
    "            new_centers = np.array([np.mean(X[z==i], axis=0) for i in range(k)])\n",
    "            delta = np.sum((new_centers - centers)**2)\n",
    "            centers = new_centers\n",
    "        score = m[z].sum()\n",
    "        if score < best_score:\n",
    "            best_score = score\n",
    "            best_z = z\n",
    "            best_centers = centers\n",
    "    return (best_z, best_centers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2018)\n",
    "z, centers = my_kemans_3(X, nc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0], X[:, 1], s=5, c=z,\n",
    "            cmap=plt.cm.get_cmap('Accent', nc))\n",
    "plt.scatter(centers[:, 0], centers[:, 1], marker='x', \n",
    "            linewidth=3, s=100, c='red')\n",
    "plt.axis('square')\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model selection\n",
    "\n",
    "We use AMIS for model selection. You can also use likelihood-based methods by interpreting the solution as a mixture of normals but we don't cover those approaches here. There are also other ad-hoc methods - see `sckikt-lean` [docs](http://scikit-learn.org/stable/modules/clustering.html#clustering-evaluation) if you are interested. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mutual Information Score (MIS)\n",
    "\n",
    "The mutual information is defined as\n",
    "\n",
    "$$\n",
    "I(X; Y) = \\sum_{y \\in Y} \\sum_{x \\in X} p(x,y) \\log \\left( \\frac{p(x, y)}{p(x)p(y)} \\right)\n",
    "$$\n",
    "\n",
    "It measures how much knowing $X$ reduces the uncertainty about $Y$ (and vice versa). \n",
    "\n",
    "- If $X$ is independent of $Y$, then $I(X; Y) = 0$\n",
    "- If $X$ is a deterministic function of $Y$, then $I(X; Y) = 1$\n",
    "\n",
    "It is equivalent to the Kullback-Leibler divergence\n",
    "\n",
    "$$\n",
    "\\text{KL}(p(x,y) \\mid\\mid p(x)p(y)\n",
    "$$\n",
    "\n",
    "We use AMIS (Adjusted MIS) here, which adjusts for the number of clusters used in the clustering method.\n",
    "\n",
    "From the documentation\n",
    "\n",
    "```\n",
    "AMI(U, V) = [MI(U, V) - E(MI(U, V))] / [max(H(U), H(V)) - E(MI(U, V))]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncols = 4\n",
    "nrows = 2\n",
    "plt.figure(figsize=(ncols*3, nrows*3))\n",
    "for i, nc in enumerate(range(3, 11)):\n",
    "    kmeans = KMeans(nc, n_init=10)\n",
    "    clusters = kmeans.fit_predict(X)\n",
    "    centers = kmeans.cluster_centers_\n",
    "    amis = adjusted_mutual_info_score(y, clusters)\n",
    "    plt.subplot(nrows, ncols, i+1)\n",
    "    plt.scatter(X[:, 0], X[:, 1], s=5, c=y,\n",
    "                cmap=plt.cm.get_cmap('Accent', nc))\n",
    "    plt.scatter(centers[:, 0], centers[:, 1], marker='x', \n",
    "                linewidth=3, s=100, c='red')\n",
    "    plt.text(0.15, 0.9, 'nc = %d AMIS = %.2f' % (nc, amis), \n",
    "             fontsize=16, color='yellow',\n",
    "             transform=plt.gca().transAxes)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.axis('square')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing across samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = X + np.random.normal(0, 1, X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 4))\n",
    "for i, xs in enumerate([X, X1]):\n",
    "    plt.subplot(1,2,i+1)\n",
    "    plt.scatter(xs[:, 0], xs[:, 1], s=5, c=y,\n",
    "                cmap=plt.cm.get_cmap('Accent', nc))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.axis('square')\n",
    "plt.tight_layout()\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 1: Using reference sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nc = 6\n",
    "kmeans = KMeans(nc, n_init=10)\n",
    "kmeans.fit(X)\n",
    "z1 = kmeans.predict(X)\n",
    "z2 = kmeans.predict(X1)\n",
    "zs = [z1, z2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 4))\n",
    "for i, xs in enumerate([X, X1]):\n",
    "    plt.subplot(1,2,i+1)\n",
    "    plt.scatter(xs[:, 0], xs[:, 1], s=5, c=zs[i],\n",
    "                cmap=plt.cm.get_cmap('Accent', nc))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.axis('square')\n",
    "plt.tight_layout()\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2: Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = np.r_[X, X1]\n",
    "kmeans = KMeans(nc, n_init=10)\n",
    "kmeans.fit(Y)\n",
    "zs = np.split(kmeans.predict(Y), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 4))\n",
    "for i, xs in enumerate([X, X1]):\n",
    "    plt.subplot(1,2,i+1)\n",
    "    plt.scatter(xs[:, 0], xs[:, 1], s=5, c=zs[i],\n",
    "                cmap=plt.cm.get_cmap('Accent', nc))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.axis('square')\n",
    "plt.tight_layout()\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 3: Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import linear_sum_assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2018)\n",
    "kmeans = KMeans(nc, n_init=10)\n",
    "c1 = kmeans.fit(X).cluster_centers_\n",
    "z1 = kmeans.predict(X)\n",
    "c2 = kmeans.fit(X1).cluster_centers_\n",
    "z2 = kmeans.predict(X1)\n",
    "zs = [z1, z2]\n",
    "cs = [c1, c2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 4))\n",
    "for i, xs in enumerate([X, X1]):\n",
    "    plt.subplot(1,2,i+1)\n",
    "    z = zs[i]\n",
    "    c = cs[i]\n",
    "\n",
    "    plt.scatter(xs[:, 0], xs[:, 1], s=5, c=z,\n",
    "                cmap=plt.cm.get_cmap('Accent', nc))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.axis('square')\n",
    "plt.tight_layout()\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define cost function\n",
    "\n",
    "We use a simple cost as just the distance between centers. More complex dissimilarity measures could be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = cdist(c1, c2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the Hungarian (Muunkres) algorithm for bipartitie matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_ind, col_ind = linear_sum_assignment(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_ind, col_ind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Swap cluster indexes to align data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z1_aligned = col_ind[z1]\n",
    "zs = [z1_aligned, z2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 4))\n",
    "for i, xs in enumerate([X, X1]):\n",
    "    plt.subplot(1,2,i+1)\n",
    "    plt.scatter(xs[:, 0], xs[:, 1], s=5, c=zs[i],\n",
    "                cmap=plt.cm.get_cmap('Accent', nc))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.axis('square')\n",
    "plt.tight_layout()\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probabilistic alternatives to k-means\n",
    "\n",
    "There are several advantages to fitting a probabilistic model:\n",
    "\n",
    "- There is a probabilistic interpretation\n",
    "- It is generative\n",
    "- You have a log likelihood to work with - e.g. evaluate model fit with AIC and BIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model selection\n",
    "\n",
    "We can use so-called information criteria that approximate the leave-one-out error. Two commonly used are \n",
    "\n",
    "- AIC - Akaike Information Criteria\n",
    "- BIC - Bayesian Information Criteria\n",
    "\n",
    "Basically, they add a penalty term to the negative log likelihood to penalize for model complexity. BIC is generally more conservative than AIC (i.e. BIC prefers less complex models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aics = []\n",
    "bics = []\n",
    "for i in range(4, 10):\n",
    "    gmm = GaussianMixture(n_components=i)\n",
    "    gmm.fit(X)\n",
    "    aics.append(gmm.aic(X))\n",
    "    bics.append(gmm.bic(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min(list(zip(range(4,10), aics)), key = lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(4, 10), aics, '-o');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min(list(zip(range(4,10), bics)), key = lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(4, 10), bics, '-o');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generative model\n",
    "\n",
    "You can draw synthetic samples from the fitted distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm = GaussianMixture(n_components=nc)\n",
    "gmm.fit(X)\n",
    "xs, zs = gmm.sample(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(xs[:, 0], xs[:, 1], s=5, c=zs,\n",
    "            cmap=plt.cm.get_cmap('Accent', nc))\n",
    "plt.axis('square')\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
