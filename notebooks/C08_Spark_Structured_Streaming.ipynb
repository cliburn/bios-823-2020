{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Structured Streaming\n",
    "\n",
    "- Data set\n",
    "- Review of Spark DataFrames and SQL\n",
    "- Concepts of structured streaming\n",
    "- Sources and sinks\n",
    "- Stateless operations\n",
    "- Stateful operations\n",
    "- Output modes\n",
    "- Windowed operations\n",
    "- Watermarks\n",
    "- Joining with streams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data set\n",
    "\n",
    "We will work with the [Heterogeneity Activity Recognition Data Set](https://archive.ics.uci.edu/ml/datasets/Heterogeneity+Activity+Recognition) from the UCI Machine Learning Repository.  A summary of the dataset from the repository:\n",
    "\n",
    "\n",
    "\n",
    "The activity recognition data set contains the readings of two motion sensors commonly found in smartphones. Reading were recorded while users executed activities scripted in no specific order carrying smartwatches and smartphones.\n",
    "\n",
    "- Activities: ‘Biking’, ‘Sitting’, ‘Standing’, ‘Walking’, ‘Stair Up’ and ‘Stair down’.\n",
    "- Sensors: Sensors: Two embedded sensors, i.e., Accelerometer and Gyroscope, sampled at the highest frequency the respective device allows.\n",
    "- Devices: 4 smartwatches (2 LG watches, 2 Samsung Galaxy Gears)\n",
    "8 smartphones (2 Samsung Galaxy S3 mini, 2 Samsung Galaxy S3, 2 LG Nexus 4, 2 Samsung Galaxy S+)\n",
    "- Recordings: 9 users \n",
    "\n",
    "The data consists of the following fields :\n",
    "\n",
    "'Index', 'Arrival_Time', 'Creation_Time', 'x', 'y', 'z', 'User', 'Model', 'Device', 'gt' \n",
    "\n",
    "where `gt` is the ground truth (activity label) and x, y, z are the motion sensor readings. The Model refers to the wearable used, while the Device refers to which sensor (accelerometer or gyroscope) the readings came from.\n",
    "\n",
    "This is used as an example since the analysis of streaming data from wearables (e.g. a medical emergency) is an important use case for streaming data in medicine. The data has been converted to a JSON format and the 6 million plus observations stored across 80 files.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to the data to experiment with:\n",
    "\n",
    "```bash\n",
    "wget https://www.dropbox.com/s/8zgrpu4o3nqdcgh/activity-data.zip\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.streaming import StreamingContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (\n",
    "    SparkSession.builder \n",
    "    .master(\"local\") \n",
    "    .appName(\"BIOS-823\") \n",
    "    .config(\"spark.executor.cores\", 4) \n",
    "    .getOrCreate()    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review of standard Spark processing with DataFrames and SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not runnig as it takes a while.\n",
    "\n",
    "```python\n",
    "spark.read.json('json/activity-data').count()\n",
    "\n",
    "Out[3]: 6240991\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read one file\n",
    "\n",
    "Note the use of schema inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "uri = 'json/activity-data/part-00000-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.json(uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Arrival_Time: long (nullable = true)\n",
      " |-- Creation_Time: long (nullable = true)\n",
      " |-- Device: string (nullable = true)\n",
      " |-- Index: long (nullable = true)\n",
      " |-- Model: string (nullable = true)\n",
      " |-- User: string (nullable = true)\n",
      " |-- gt: string (nullable = true)\n",
      " |-- x: double (nullable = true)\n",
      " |-- y: double (nullable = true)\n",
      " |-- z: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+--------+-----+------+----+-----+-------------+------------+-------------+\n",
      "| Arrival_Time|Creation_Time|  Device|Index| Model|User|   gt|            x|           y|            z|\n",
      "+-------------+-------------+--------+-----+------+----+-----+-------------+------------+-------------+\n",
      "|1424686735000|1424686733000|nexus4_1|   35|nexus4|   g|stand| 0.0014038086|   5.0354E-4|-0.0124053955|\n",
      "|1424686735000|1424686733000|nexus4_1|   76|nexus4|   g|stand|-0.0039367676| 0.026138306|  -0.01133728|\n",
      "|1424686736000|1424686734000|nexus4_1|  115|nexus4|   g|stand|  0.003540039|-0.034744263| -0.019882202|\n",
      "+-------------+-------------+--------+-----+------+----+-----+-------------+------------+-------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView('activity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+--------+-----+------+----+-----+-------------+------------+-------------+\n",
      "| Arrival_Time|Creation_Time|  Device|Index| Model|User|   gt|            x|           y|            z|\n",
      "+-------------+-------------+--------+-----+------+----+-----+-------------+------------+-------------+\n",
      "|1424686735000|1424686733000|nexus4_1|   35|nexus4|   g|stand| 0.0014038086|   5.0354E-4|-0.0124053955|\n",
      "|1424686735000|1424686733000|nexus4_1|   76|nexus4|   g|stand|-0.0039367676| 0.026138306|  -0.01133728|\n",
      "|1424686736000|1424686734000|nexus4_1|  115|nexus4|   g|stand|  0.003540039|-0.034744263| -0.019882202|\n",
      "+-------------+-------------+--------+-----+------+----+-----+-------------+------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT * FROM activity LIMIT 3').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stateless operations\n",
    "\n",
    "Stateless operations process one row at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+--------+-----+------+----+-----+------------+-------------+------------+\n",
      "| Arrival_Time|Creation_Time|  Device|Index| Model|User|   gt|           x|            y|           z|\n",
      "+-------------+-------------+--------+-----+------+----+-----+------------+-------------+------------+\n",
      "|1424696634000|1424696632000|nexus4_1|   22|nexus4|   a|stand|0.0062713623| -0.013442993|-0.009490967|\n",
      "|1424696634000|1424696632000|nexus4_1|   78|nexus4|   a|stand|-0.040725708| -0.016647339| 0.072753906|\n",
      "|1424696635000|1424698481000|nexus4_2|   49|nexus4|   a|stand| -0.04573059|-0.0140686035|  0.08921814|\n",
      "+-------------+-------------+--------+-----+------+----+-----+------------+-------------+------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(F.expr('User')=='a').show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Time conversion \n",
    "\n",
    "Convert from Unix epoch to standard format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+\n",
      "|Arrival              |\n",
      "+---------------------+\n",
      "|+47116-07-11 15:16:40|\n",
      "|+47116-07-11 15:16:40|\n",
      "|+47116-07-11 15:33:20|\n",
      "+---------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    df.select(F.from_unixtime(F.expr('Arrival_Time'),\n",
    "                              'yyyy-MM-dd HH:mm:ss').alias('Arrival'))             \n",
    ").show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stateful operations\n",
    "\n",
    "Need to aggregate data over multiple rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+--------------------+--------------------+--------------------+\n",
      "|User|        gt|              avg(x)|              avg(y)|              avg(z)|\n",
      "+----+----------+--------------------+--------------------+--------------------+\n",
      "|   a|      bike| 0.08286326523323344|-0.02426324084800...| -0.1662348217003992|\n",
      "|   a|      null|-0.01308972796540...|0.016658997597921752| 0.04942386999523227|\n",
      "|   a|       sit|-2.12969390963855...|-1.50075401673359...|0.001048883435073...|\n",
      "|   a|stairsdown| 0.23440874162122333|-0.06719274877160608| -0.3077443049653919|\n",
      "|   a|  stairsup|-0.29239928706342583|-0.00677555257509...|       0.31678605306|\n",
      "|   a|     stand|-0.00177360789606...|0.001089580071408...|0.001844132059828448|\n",
      "|   a|      walk|0.019868167564847505|-0.02196182985947...|-0.01037372278081...|\n",
      "|   b|      bike|0.007747435248372937| 0.01460614637146433|-0.02048030271908635|\n",
      "|   b|      null|0.006628031256594217|-0.01005675819492...|0.024657572641956556|\n",
      "|   b|       sit|7.738246112637349E-4|0.001047714316483...|-0.00112005380650...|\n",
      "|   b|stairsdown| -0.1337819220130277|-0.03978477199688...|  0.4430138123083492|\n",
      "|   b|  stairsup|  0.1908027992841166|-0.04446313425753646|-0.36332664294651545|\n",
      "|   b|     stand|-2.50695839259927...|-1.48484005595668...|-0.00105500472075...|\n",
      "|   b|      walk|-0.00202879781863...|-0.02067295743016658|-0.00539236749586...|\n",
      "|   c|      bike|0.048871796901853976|-0.01610419329918887|-0.14889040097647727|\n",
      "|   c|      null|-0.00153568544845...|-0.00134610721863...|0.001905112306683...|\n",
      "|   c|       sit|-5.94906131651372...|-3.68963991513762...|0.001000477139678...|\n",
      "|   c|stairsdown|  0.2281934003521868|-0.00901284850815...|-0.26389999844155076|\n",
      "|   c|  stairsup| -0.2585194447920258|-0.01185626514694...|  0.3040023440573634|\n",
      "|   c|     stand|-0.00204057011630716|0.001435063434771...|0.001349290326747...|\n",
      "+----+----------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    df.groupBy('User', 'gt').mean('x', 'y', 'z').sort('User', 'gt')\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structured Streaming Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming data\n",
    "\n",
    "Each time a *trigger* occurs, new data is read into the stream.\n",
    "\n",
    "- Minibatch - a new read is triggered once the previous minibatch is processed (default)\n",
    "- Fixed interval - e.g. every 5 minutes\n",
    "- One-time - spin up cluster, process all data, then stop (economical for infrequent jobs)\n",
    "- Continuous (for low latency applications ~ 1 ms c.f. minibatch ~ 100 ms)\n",
    "\n",
    "![img](https://spark.apache.org/docs/latest/img/structured-streaming-model.png)\n",
    "Source: https://spark.apache.org/docs/latest/img/structured-streaming-model.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query on a stream is conceptually an unbounded table updated in mini-batches\n",
    "\n",
    "![img](https://tse2.mm.bing.net/th?id=OIP.sMrdnOlx6YJdnl6DU8RyswHaDz&pid=Api&w=1037&h=533&rs=1&p=0)\n",
    "\n",
    "Reference: [Structured Streaming Programming Guide](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sources and sinks\n",
    "\n",
    "#### Sources\n",
    "\n",
    "- Files\n",
    "- Kafka (an open-source distributed event streaming platform)\n",
    "- Socket (for testing)\n",
    "- Rate (for testing)\n",
    "\n",
    "#### Sinks\n",
    "\n",
    "- File\n",
    "- Kafka\n",
    "- Console (for testing)\n",
    "- Memory (for testing)\n",
    "- Foreach sink (Runs arbitrary computation on the records in the output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### States\n",
    "\n",
    "![img](https://cdn2.hubspot.net/hubfs/4757017/Imported_Blog_Media/state-1.png)\n",
    "Source: https://cdn2.hubspot.net/hubfs/4757017/Imported_Blog_Media/state-1.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output modes\n",
    "\n",
    "- `append` is useful for stateless operations\n",
    "- `complete` and `update` modes are useful for stateful operations\n",
    "\n",
    "![img](https://vishnuviswanath.com/img/spark_structured_streaming/output_modes.png)\n",
    "Source: https://vishnuviswanath.com/img/spark_structured_streaming/output_modes.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Windowed operations\n",
    "\n",
    "![img](https://spark.apache.org/docs/latest/img/structured-streaming-window.png)\n",
    "Source: https://spark.apache.org/docs/latest/img/structured-streaming-window.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Watermarks\n",
    "\n",
    "There are two relevant times - the time an event is generated (creation time) and the time the event data is received (arrival time). The arrival time may be much later than the creation time for various reasons. Hence windows that define start and end times for *events* must continue to aggregate data long after the stipulated window period. To avoid excessive use of storage, *watermarks* can be defined to indicate that events that arrive beyond some threshold will be ignored.\n",
    "\n",
    "![img](https://spark.apache.org/docs/latest/img/structured-streaming-watermark-append-mode.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joining\n",
    "\n",
    "You can join Spark streams to another stream, or to a static DataFrame. For example, you may have a patient with 2 independent sensors (e.g. ECG, HR) as streaming sources, and you want to combine the information. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](https://cdn-images-1.medium.com/max/800/1*Qb5RmfVt6XYVYGonYoBiuA.png)\n",
    "Source: https://cdn-images-1.medium.com/max/800/1*Qb5RmfVt6XYVYGonYoBiuA.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structured Streaming code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Streaming does not infer schmea by default, so we read in a single file statically first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.json(uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Arrival_Time: long (nullable = true)\n",
      " |-- Creation_Time: long (nullable = true)\n",
      " |-- Device: string (nullable = true)\n",
      " |-- Index: long (nullable = true)\n",
      " |-- Model: string (nullable = true)\n",
      " |-- User: string (nullable = true)\n",
      " |-- gt: string (nullable = true)\n",
      " |-- x: double (nullable = true)\n",
      " |-- y: double (nullable = true)\n",
      " |-- z: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We fix the schema to use timestamps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import (\n",
    "    StructType, \n",
    "    StructField, \n",
    "    LongType, \n",
    "    StringType, \n",
    "    TimestampType, \n",
    "    DoubleType\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "        StructField('Arrival_Time',TimestampType(),True),\n",
    "        StructField('Creation_Time',TimestampType(),True),\n",
    "        StructField('Device',StringType(),True),\n",
    "        StructField('Index',LongType(),True),\n",
    "        StructField('Model',StringType(),True),\n",
    "        StructField('User',StringType(),True),\n",
    "        StructField('gt',StringType(),True),\n",
    "        StructField('x',DoubleType(),True),\n",
    "        StructField('y',DoubleType(),True),\n",
    "        StructField('z',DoubleType(),True)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating an input stream from files\n",
    "\n",
    "The `maxFilesPerTrigger` option shown forces the stream to read one file per trigger. This is only used in testing to simulate the arrival of new files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream = (\n",
    "    spark.readStream.\n",
    "    schema(schema).\n",
    "    option('maxFilesPerTrigger', 1).\n",
    "    json('json/activity-data/')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can operate on the stream almost like a regular DataFrame. Note that we canno simply `show` the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = stream.groupby('gt').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating output\n",
    "\n",
    "Here the sink is a table in `memory` and we must assign the table a name so that it can be queried. We also specify the output mode as `complete`, which means that it will regenerate the full data frame each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = (\n",
    "    counts.writeStream.\n",
    "    queryName('activity_counts_complete').\n",
    "    format('memory').\n",
    "    outputMode('complete').\n",
    "    start()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that there is an active sgtrem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<pyspark.sql.streaming.StreamingQuery at 0x10b4e4a60>]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.streams.active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that the output is being updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|        gt|count|\n",
      "+----------+-----+\n",
      "|  stairsup|20905|\n",
      "|       sit|24619|\n",
      "|     stand|22769|\n",
      "|      walk|26512|\n",
      "|      bike|21593|\n",
      "|stairsdown|18729|\n",
      "|      null|20896|\n",
      "+----------+-----+\n",
      "\n",
      "+----------+-----+\n",
      "|        gt|count|\n",
      "+----------+-----+\n",
      "|  stairsup|31357|\n",
      "|       sit|36929|\n",
      "|     stand|34154|\n",
      "|      walk|39768|\n",
      "|      bike|32390|\n",
      "|stairsdown|28094|\n",
      "|      null|31343|\n",
      "+----------+-----+\n",
      "\n",
      "+----------+-----+\n",
      "|        gt|count|\n",
      "+----------+-----+\n",
      "|  stairsup|41809|\n",
      "|       sit|49238|\n",
      "|     stand|45539|\n",
      "|      walk|53024|\n",
      "|      bike|43187|\n",
      "|stairsdown|37459|\n",
      "|      null|41791|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    spark.sql('''\n",
    "    SELECT * from activity_counts_complete\n",
    "    ''').show()\n",
    "    time.sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformations on streams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = (\n",
    "    stream.withColumn(\"stairs\", F.expr(\"gt like '%stairs%'\")).\n",
    "    where(\"stairs\").\n",
    "    where(\"gt is not null\").\n",
    "    select(\"gt\", \"model\", \"arrival_time\", \"creation_time\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = (\n",
    "    s1.writeStream.\n",
    "    queryName(\"transform_example\").\n",
    "    format(\"memory\").\n",
    "    outputMode(\"update\").\n",
    "    start()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+--------------------+--------------------+\n",
      "|      gt| model|        arrival_time|       creation_time|\n",
      "+--------+------+--------------------+--------------------+\n",
      "|stairsup|nexus4|+47116-07-26 02:1...|+47116-07-26 01:4...|\n",
      "|stairsup|nexus4|+47116-07-26 02:1...|+47116-07-26 01:4...|\n",
      "|stairsup|nexus4|+47116-07-26 02:1...|+47116-07-26 01:4...|\n",
      "|stairsup|nexus4|+47116-07-26 02:3...|+47116-07-26 01:5...|\n",
      "|stairsup|nexus4|+47116-07-26 02:3...|+47116-07-26 01:5...|\n",
      "|stairsup|nexus4|+47116-07-26 02:4...|+47116-07-26 02:1...|\n",
      "|stairsup|nexus4|+47116-07-26 02:4...|+47116-07-26 02:1...|\n",
      "|stairsup|nexus4|+47116-07-26 02:4...|+47116-07-26 02:1...|\n",
      "|stairsup|nexus4|+47116-07-26 03:0...|+47116-07-26 02:3...|\n",
      "|stairsup|nexus4|+47116-07-26 03:0...|+47116-07-26 02:3...|\n",
      "|stairsup|nexus4|+47116-07-26 03:2...|+47116-07-26 02:4...|\n",
      "|stairsup|nexus4|+47116-07-26 03:2...|+47116-07-26 02:4...|\n",
      "|stairsup|nexus4|+47116-07-26 03:2...|+47116-08-16 12:0...|\n",
      "|stairsup|nexus4|+47116-07-26 03:2...|+47116-08-16 12:0...|\n",
      "|stairsup|nexus4|+47116-07-26 03:3...|+47116-08-16 12:2...|\n",
      "|stairsup|nexus4|+47116-07-26 03:3...|+47116-08-16 12:2...|\n",
      "|stairsup|nexus4|+47116-07-26 03:3...|+47116-08-16 12:2...|\n",
      "|stairsup|nexus4|+47116-07-26 03:3...|+47116-07-26 03:0...|\n",
      "|stairsup|nexus4|+47116-07-26 03:3...|+47116-08-16 12:2...|\n",
      "|stairsup|nexus4|+47116-07-26 03:5...|+47116-07-26 03:2...|\n",
      "+--------+------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "SELECT * FROM transform_example\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2 = (\n",
    "    stream.groupby(\"gt\").mean('x', 'y', 'z')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = (\n",
    "    s2.writeStream.\n",
    "    queryName(\"agg_example\").\n",
    "    format(\"memory\").\n",
    "    outputMode(\"complete\").\n",
    "    start()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+--------------------+--------------------+\n",
      "|        gt|              avg(x)|              avg(y)|              avg(z)|\n",
      "+----------+--------------------+--------------------+--------------------+\n",
      "|  stairsup|-0.02623301318863...|-0.01385931765291...|-0.09395009728019467|\n",
      "|       sit|-4.92132825379802...|3.757376157445754...|-4.42863346250723E-5|\n",
      "|     stand|-3.00989804137386...|4.133303473120163...|-2.86960196767408...|\n",
      "|      walk|0.001970352086338...|7.489666845955465E-5|-0.00149828380428...|\n",
      "|      bike|0.023512544470933695|-0.01304747996973...|-0.08360475809007037|\n",
      "|stairsdown|0.028103791071500146|-0.03570080351911368| 0.12203047970606441|\n",
      "|      null|-0.00302501221506...|-0.00410754501410...|0.005961452067049526|\n",
      "+----------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "SELECT * FROM agg_example\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Windowed operations with watermarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = (\n",
    "    stream.\n",
    "    withWatermark('Creation_Time', '30 minute').\n",
    "    groupBy('User', F.window('Creation_Time', '10 minute', '5 minute')).\n",
    "    count()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = (\n",
    "    s3.writeStream.\n",
    "    queryName(\"windowed_example\").\n",
    "    format(\"memory\").\n",
    "    outputMode(\"complete\").\n",
    "    start()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+-----+\n",
      "|User|window|count|\n",
      "+----+------+-----+\n",
      "+----+------+-----+\n",
      "\n",
      "+----+--------------------+-----+\n",
      "|User|              window|count|\n",
      "+----+--------------------+-----+\n",
      "|   h|[+47116-12-27 22:...|    2|\n",
      "|   f|[+47119-07-10 13:...|    1|\n",
      "|   d|[+47119-08-09 14:...|    2|\n",
      "|   a|[+47116-12-01 15:...|    4|\n",
      "|   f|[+47119-07-29 01:...|    3|\n",
      "|   b|[+47119-09-14 04:...|    2|\n",
      "|   i|[+47119-06-29 19:...|    4|\n",
      "|   e|[+47119-10-28 23:...|    3|\n",
      "|   d|[+47119-07-21 02:...|    4|\n",
      "|   a|[+47116-11-27 18:...|    3|\n",
      "|   e|[+47119-09-17 09:...|    2|\n",
      "|   f|[+47119-06-23 16:...|    2|\n",
      "|   b|[+47119-09-09 10:...|    4|\n",
      "|   c|[+47116-10-23 23:...|    1|\n",
      "|   c|[+47116-10-20 19:...|    3|\n",
      "|   d|[+47119-08-22 07:...|    3|\n",
      "|   a|[+47116-11-24 23:...|    1|\n",
      "|   g|[+47116-08-19 21:...|    3|\n",
      "|   i|[+47119-05-24 09:...|    2|\n",
      "|   f|[+47119-07-13 10:...|    1|\n",
      "+----+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "+----+--------------------+-----+\n",
      "|User|              window|count|\n",
      "+----+--------------------+-----+\n",
      "|   f|[+47119-07-10 13:...|    4|\n",
      "|   a|[+47116-12-01 15:...|    7|\n",
      "|   f|[+47119-07-29 01:...|    6|\n",
      "|   i|[+47119-06-29 19:...|    6|\n",
      "|   h|[+47116-12-09 09:...|    4|\n",
      "|   e|[+47119-09-17 09:...|    5|\n",
      "|   b|[+47119-09-09 10:...|   12|\n",
      "|   c|[+47116-10-20 19:...|    4|\n",
      "|   d|[+47119-08-22 07:...|    4|\n",
      "|   a|[+47116-11-24 23:...|    1|\n",
      "|   g|[+47116-08-19 21:...|    5|\n",
      "|   g|[+47116-08-15 01:...|    5|\n",
      "|   d|[+47119-07-25 16:...|    4|\n",
      "|   i|[+47119-05-29 05:...|    5|\n",
      "|   a|[+47116-11-29 07:...|    3|\n",
      "|   c|[+47116-11-05 17:...|    6|\n",
      "|   g|[+47116-07-17 11:...|    6|\n",
      "|   g|[+47116-08-15 13:...|    4|\n",
      "|   e|[+47119-10-12 13:...|    2|\n",
      "|   g|[+47116-07-18 09:...|    4|\n",
      "+----+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    spark.sql('''\n",
    "    SELECT * FROM windowed_example\n",
    "    ''').show()\n",
    "    time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
