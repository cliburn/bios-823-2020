{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (\n",
    "    SparkSession.builder \n",
    "    .master(\"local\") \n",
    "    .appName(\"BIOS-823\") \n",
    "    .config(\"spark.executor.cores\", 4) \n",
    "    .getOrCreate()    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(pd.DataFrame(dict(a=range(6), b=list('aabbcc'))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|  a|  b|\n",
      "+---+---+\n",
      "|  0|  a|\n",
      "|  1|  a|\n",
      "|  2|  b|\n",
      "|  3|  b|\n",
      "|  4|  c|\n",
      "|  5|  c|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView('df_table')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard SQL Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|  a|  b|\n",
      "+---+---+\n",
      "|  0|  a|\n",
      "|  1|  a|\n",
      "|  2|  b|\n",
      "|  3|  b|\n",
      "|  4|  c|\n",
      "|  5|  c|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "SELECT * FROM df_table\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|  a|  b|\n",
      "+---+---+\n",
      "|  2|  b|\n",
      "|  3|  b|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "SELECT * \n",
    "FROM df_table\n",
    "WHERE b='b'\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+------------------+\n",
      "|  b|avg|               std|\n",
      "+---+---+------------------+\n",
      "|  a|0.5|0.7071067811865476|\n",
      "|  b|2.5|0.7071067811865476|\n",
      "|  c|4.5|0.7071067811865476|\n",
      "+---+---+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "SELECT b, mean(a) AS avg, std(a) AS std\n",
    "FROM df_table\n",
    "GROUP BY b\n",
    "ORDER BY b\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|        function|\n",
      "+----------------+\n",
      "|     cardinality|\n",
      "|            case|\n",
      "|            cast|\n",
      "|            cbrt|\n",
      "|            ceil|\n",
      "|         ceiling|\n",
      "|            char|\n",
      "|     char_length|\n",
      "|character_length|\n",
      "|             chr|\n",
      "+----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "SHOW FUNCTIONS \"c*\"\n",
    "''').show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complex Types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = spark.sql('''\n",
    "SELECT a, b, (a, b) AS struct FROM df_table\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+------+\n",
      "|  a|  b|struct|\n",
      "+---+---+------+\n",
      "|  0|  a|[0, a]|\n",
      "|  1|  a|[1, a]|\n",
      "|  2|  b|[2, b]|\n",
      "|  3|  b|[3, b]|\n",
      "|  4|  c|[4, c]|\n",
      "|  5|  c|[5, c]|\n",
      "+---+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.createOrReplaceTempView('df1_table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|  a|  b|\n",
      "+---+---+\n",
      "|  0|  a|\n",
      "|  1|  a|\n",
      "|  2|  b|\n",
      "|  3|  b|\n",
      "|  4|  c|\n",
      "|  5|  c|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "SELECT struct.a, struct.b FROM df1_table\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = spark.sql('''\n",
    "SELECT b, collect_list(a) as list\n",
    "FROM df_table\n",
    "GROUP BY b\n",
    "ORDER BY b\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.createOrReplaceTempView('df2_table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "|  b|  list|\n",
      "+---+------+\n",
      "|  a|[0, 1]|\n",
      "|  b|[2, 3]|\n",
      "|  c|[4, 5]|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|  b|  a|\n",
      "+---+---+\n",
      "|  a|  0|\n",
      "|  a|  1|\n",
      "|  b|  2|\n",
      "|  b|  3|\n",
      "|  c|  4|\n",
      "|  c|  5|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "SELECT b, explode(list) as a\n",
    "FROM df2_table\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import create_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df.select('a', 'b', create_map('b', 'a').alias('map'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.createOrReplaceTempView('df3_table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+--------+\n",
      "|  a|  b|     map|\n",
      "+---+---+--------+\n",
      "|  0|  a|[a -> 0]|\n",
      "|  1|  a|[a -> 1]|\n",
      "|  2|  b|[b -> 2]|\n",
      "|  3|  b|[b -> 3]|\n",
      "|  4|  c|[c -> 4]|\n",
      "|  5|  c|[c -> 5]|\n",
      "+---+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+------+\n",
      "|  a|  b|map[c]|\n",
      "+---+---+------+\n",
      "|  4|  c|     4|\n",
      "|  5|  c|     5|\n",
      "+---+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "SELECT a, b, map['c'] FROM df3_table\n",
    "''').na.drop().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+\n",
      "|  a|  b|  c|\n",
      "+---+---+---+\n",
      "|  4|  c|  4|\n",
      "|  5|  c|  5|\n",
      "+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.select('a', 'b', 'map.c').filter('c is not null').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit",
   "language": "python",
   "name": "python38564bit02a66c47ce504b05b2ef5646cfed96c2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
