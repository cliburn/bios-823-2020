{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this on the `vm-managee` cluster.\n",
    "```\n",
    "%%spark\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "sc =  SparkContext.getOrCreate()\n",
    "spark = (\n",
    "    SparkSession.builder \n",
    "    .master(\"local\") \n",
    "    .appName(\"BIOS-823\") \n",
    "    .config(\"spark.executor.cores\", 4) \n",
    "    .getOrCreate()    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1**. Low-level API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create an RDD with numbers from 1 to 100\n",
    "- Remove all numbers not divisible by 3\n",
    "- Square the numbers\n",
    "- Find the remainder modulo 7\n",
    "- Count the frequency of each digit from 0-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2**. High-level API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "data, target = load_iris(return_X_y=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Combine the data and target variables into a pandas DataFrame with columns \n",
    "['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'species']\n",
    "- Convert into a Spark DataFrame\n",
    "- Find the median and IQR for all measurements by `species`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3**. Spark ML\n",
    "\n",
    "In this exercise you will use Spark to build and run a machine learning pipeline to separate 'ham' from 'spam' in SMS text messages. Then you will use the pipeline to classify SMS texts.\n",
    "\n",
    "- Create a Pandas DataFraem from the data in the file`SMSSpamCollection` where each line is tab separated into (label, text). If you find that the read_xxx function in Pandas does not do the job correctly, read in the file line by line before converting to a DataFrame. Create an index column so that each row has a unique number id.\n",
    "- Convert to a Spark DataFrame that has two columns (klass, SMS) and split into test and training data sets with proportions 0.8 and 0.2 respectively using a random seed of 123.\n",
    "- Build a Spark ML pipeline consisting of the following \n",
    "    - StringIndexer: To convert `klass` into a numeric `labels` column\n",
    "    - Tokenizer: To covert `SMS` into a list of tokens\n",
    "    - StopWordsRemover: To remove \"stop words\" from the tokens\n",
    "    - CountVectorizer: To count words (use a vocabular size of 100 and minimum number of occureences of 2)\n",
    "    - LogisticRegression: Use `maxIter=10`, `regParam=0.001`\n",
    "\n",
    "- Train the model on the test data.\n",
    "- Evaluate the precision, recall and accuracy of this model on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4**.  Spark Streaming\n",
    "\n",
    "In this exercise, you will simulate running a machine learning pipeline to classify steaming data.\n",
    "\n",
    "- Convert the test DataFrame into a Pandas DataFrame\n",
    "- Write each row of the DataFrame to a separate tab-delimited file in a folder called \"incoming_sms\"\n",
    "- Create a Structured Streaming DataFrame using `readStream` with `option(\"maxFilesPerTrigger\", 1)` to simulate streaming data\n",
    "- Use the fitted pipeline created in Ex. 1 to transform the input stream\n",
    "- Write the transformed stream to memory with name `sms_pred\n",
    "- Sleep 30 seconds\n",
    "- Use an SQL query to show the `index`, `label` and `prediction` columns\n",
    "- Sleep 30 more seconds\n",
    "- Use an SQL query to show the `index`, `label` and `prediction` columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit",
   "language": "python",
   "name": "python38564bit02a66c47ce504b05b2ef5646cfed96c2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
