{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = (\n",
    "    'http://biostat.mc.vanderbilt.edu/' \n",
    "    'wiki/pub/Main/DataSets/titanic3.xls'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(url)\n",
    "df_orig = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python3 -m pip install --quiet category_encoders missingno yellowbrick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import category_encoders as ce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import missingno as mn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop leaky or low-information variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns = ['survived', 'name', 'ticket', 'boat', 'body', 'cabin', 'home.dest'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mn.matrix(df);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select_dtypes('object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore', FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert categorical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.get_dummies(df, drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Impute missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = IterativeImputer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:, :] = imputer.fit_transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mn.matrix(df);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:, :] = scaler.fit_transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimension reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA\n",
    "\n",
    "If your knowledge of PCA is fading, see the notebook `B03A_PCA.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pca.fit_transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.cumsum(pca.explained_variance_ratio_)\n",
    "x = np.arange(1, len(y)+1)\n",
    "plt.bar(x, y, alpha = 0.2)\n",
    "plt.plot(x, y)\n",
    "plt.xlabel('Principal components')\n",
    "plt.ylabel('Explained variance')\n",
    "plt.title('Cumulative fraction of variance explained', fontsize=16);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orig.survived"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X = pd.DataFrame(X[:, :4], columns = [f'PC{i}' for i in range(1, 5)])\n",
    "df_X['survived'] = df_orig.survived"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df_X, hue='survived');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "plt.imshow(pca.components_.T, cmap='Spectral', vmin=-1, vmax=1)\n",
    "plt.colorbar(fraction=0.046, pad=0.04)\n",
    "plt.yticks(range(len(df.columns)), df.columns, fontsize=14);\n",
    "plt.xticks(range(len(df.columns)), 1+np.arange(len(df.columns)), fontsize=14);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.features import PCA as PCA_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['font.size'] = 14\n",
    "pca_viz = PCA_(scale=True, proj_features=True)\n",
    "pca_viz.fit_transform(df, df_orig.survived)\n",
    "pca_viz.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting the biplot\n",
    "\n",
    "Bi = PCA plot + loadings plot\n",
    "\n",
    "Each PC is just a linear combination of the original variables. \n",
    "\n",
    "$$\n",
    "v = \\alpha_1 x_1 + \\alpha_2 x_2 + \\ldots + \\alpha_n x_n\n",
    "$$\n",
    "\n",
    "The coefficients $\\alpha_i$ are known as *loadings* for each PC. This is stored in the `components_` attribute of the `sklearn` PCA instance. The loadings plot shows the contributions of the original features onto the PC axes. Here we show how `pclass` and `page` are projected as arrows onto the first 2 PC axes to make the process explicit.\n",
    "\n",
    "- Arrows that point in the same direction indicate that the corresponding features are positively correlated\n",
    "- Arrows that point in the opposite direction that the corresponding features are negatively correlated\n",
    "- Arrows that are orthogonal that the corresponding features are uncorrelated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loadings = pd.DataFrame(pca.components_.T, columns = df.columns)\n",
    "loadings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1, y1 = loadings.pclass[:2]\n",
    "x2, y2 = loadings.age[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.arrow(0,0,x1,y1, head_width=0.02)\n",
    "plt.arrow(0,0,x2,y2, head_width=0.02)\n",
    "plt.text(x1 + 0.05, y1, loadings.columns[0])\n",
    "plt.text(x2 - 0.05, y2, loadings.columns[1])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We expect these to be negatively correlated since they point in approximately opposite directions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.corr().iloc[:2, :2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other dimension reduction methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA does not preserve local structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limitations of PCA\n",
    "\n",
    "We will project a 2-d data set onto 1-d to see one limitation of PCA. This provides motivation for learning non-linear methods of dimension reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = np.random.multivariate_normal([-3,3], np.eye(2), 100)\n",
    "x2 = np.random.multivariate_normal([3,3], np.eye(2), 100)\n",
    "x3 = np.random.multivariate_normal([0,-10], np.eye(2), 100)\n",
    "xs = np.r_[x1, x2, x3]\n",
    "xs = (xs - xs.mean(0))/xs.std()\n",
    "zs = np.r_[np.zeros(100), np.ones(100), 2*np.ones(100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(xs[:, 0], xs[:, 1], c=zs, cmap='Set1')\n",
    "plt.axis('equal')\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys = pca.fit_transform(xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(ys[:, 0], np.random.uniform(-1, 1, len(ys)), c=zs, cmap='Set1')\n",
    "plt.axhline(0, c='red')\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T-SNE preserves locality\n",
    "\n",
    "The t-SNE algorithm was designed to preserve local distances between points in the original space, as we saw in the example above. This means that t-SNE is particularly effective at preserving **clusters** in the original space. The full t-SNE algorithm is quite complex, so we just sketch the ideas here.\n",
    "\n",
    "For more details, see the original [series of papers](https://lvdmaaten.github.io/tsne/) and this Python [tutorial](https://www.oreilly.com/learning/an-illustrated-introduction-to-the-t-sne-algorithm). The algorithm is also clearly laid out in the fairly comprehensive [tutorial](https://www.analyticsvidhya.com/blog/2017/01/t-sne-implementation-r-python/).\n",
    "\n",
    "### Outline of t-SNE\n",
    "\n",
    "t-SNE is similar in outline to MDS, with two main differences - \"distances\" are baased on probabilistic concepts and depend on the local neighborhood of the point.\n",
    "\n",
    "#### Original space\n",
    "\n",
    "- Find the conditinoal similarity between points in the original space based on a Gaussian kernel\n",
    "\n",
    "$$\n",
    "p_{j \\mid i} = \\frac{f(\\vert y_i - y_j \\vert)}{\\sum_{k \\ne i} {f(\\vert y_i - y_k \\vert)}}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "f(z) = {e^\\frac{{-z^2}}{2\\sigma_i^2}}\n",
    "$$\n",
    "\n",
    "- Symmetize the conditional similarity (this is necessary becasue each kernel has its own variance)\n",
    "\n",
    "$$\n",
    "p_{ij} = \\frac{p_{i \\mid j} + p_{j \\mid i}}{2}\n",
    "$$\n",
    "\n",
    "- This gives a similarity matrix $p_{ij}$ that is fixed\n",
    "\n",
    "Notes\n",
    "\n",
    "- In t-SNE, the variance of the Gaussian kernel depensd on the point $x_i$. Intuitively, we want the variance to be small if $x_i$ is in a locally desnse region, and to be large if $x_i$ is in a locally sparse region. This is done by an iteratvie algorithm that depends on a user-defined variable called **perplexity**. Roughly, perplexity determines the number of meaningful neighbors each point should have.\n",
    "\n",
    "#### Map space\n",
    "\n",
    "- Find the conditional similarity between points in the map space based on a Cauchy kernel\n",
    "\n",
    "$$\n",
    "q_{ij} = \\frac{g(\\vert y_i - y_j \\vert)}{\\sum_{k \\ne i} {g(\\vert y_i - y_k \\vert)}}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "g(z) = \\frac{1}{1+z^2}\n",
    "$$\n",
    "\n",
    "- This gives a similarity matrix $q_{ij}$ that depends on the points in the map space that we can vary\n",
    "\n",
    "#### Optimization\n",
    "\n",
    "- Minimize the Kullback-Leibler divergence between $p_{ij}$ and $q_{ij}$\n",
    "\n",
    "$$\n",
    "\\text{KL}(P \\mid\\mid  Q) = \\sum p_{ij} \\log{\\frac{p_{ij}}{q_{ij}}}\n",
    "$$\n",
    "\n",
    "#### Normal and Cauhcy distributions\n",
    "\n",
    "The Cauchy has much fatter tails than the normal distribution.  This means that two points that are widely separated in the original space would be pushed much further apart in the map space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python3 -m pip install --quiet fitsne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitsne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys = fitsne.FItSNE(xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(ys[:, 0], np.random.uniform(-1, 1, len(ys)), c=zs, cmap='Set1')\n",
    "plt.axhline(0, c='red')\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Illustrating with MNIST digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "X, y = fetch_openml('mnist_784', version=1, return_X_y=True)\n",
    "plt.imshow(X[0].reshape((28,28)), cmap='binary')\n",
    "plt.title(f'label = {y[0]}');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "sc = plt.scatter(np.arange(10), np.arange(10), c=np.arange(10), cmap='tab10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], s=1, \n",
    "            c=y.astype('int'), cmap='tab10')\n",
    "plt.title('Images in PCA space colored by label')\n",
    "for i in range(10):\n",
    "    idx = y == i\n",
    "    μ = np.mean(X_pca[y == i], axis=0)\n",
    "    plt.text(*μ, str(i), va='center', ha='center', \n",
    "             bbox=dict(facecolor='yellow', alpha=0.5))\n",
    "plt.legend(*sc.legend_elements(), \n",
    "           bbox_to_anchor=(1,1), \n",
    "           fontsize=20, \n",
    "           markerscale=2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### t-SNE preserves local structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitsne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "X = X.copy(order='C')\n",
    "X_tsne = fitsne.FItSNE(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_tsne[:, 0], X_tsne[:, 1], s=1, \n",
    "            c=y.astype('int'), cmap='tab10')\n",
    "plt.title('Images in t-SNE space colored by label')\n",
    "for i in range(10):\n",
    "    idx = y == i\n",
    "    μ = np.mean(X_tsne[y == i], axis=0)\n",
    "    plt.text(*μ, str(i), va='center', ha='center', \n",
    "             bbox=dict(facecolor='yellow', alpha=0.5))\n",
    "plt.legend(*sc.legend_elements(), \n",
    "           bbox_to_anchor=(1,1), \n",
    "           fontsize=20, \n",
    "           markerscale=2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UMAP preserves local and (maybe) global structure\n",
    "\n",
    "Normally I would refer you to the [original paper](https://arxiv.org/pdf/1802.03426). But the original paper is hard to read unless you have graduate training in pure mathematics, so visit this [tutorial](https://pair-code.github.io/understanding-umap/) instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "X_umap = umap.UMAP().fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_umap[:, 0], X_umap[:, 1], s=1, \n",
    "            c=y, cmap='tab10')\n",
    "plt.title('Images in UMAP space colored by label')\n",
    "for i in range(10):\n",
    "    idx = y == i\n",
    "    μ = np.mean(X_umap[y == i], axis=0)\n",
    "    plt.text(*μ, str(i), va='center', ha='center', \n",
    "             bbox=dict(facecolor='yellow', alpha=0.5))\n",
    "plt.legend(*sc.legend_elements(), \n",
    "           bbox_to_anchor=(1,1), \n",
    "           fontsize=20, \n",
    "           markerscale=2);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit",
   "language": "python",
   "name": "python38564bit02a66c47ce504b05b2ef5646cfed96c2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
