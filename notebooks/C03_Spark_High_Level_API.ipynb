{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark High-Level API\n",
    "\n",
    "![img](https://static.packt-cdn.com/products/9781785888359/graphics/8bffbd94-04f7-46e3-a1e9-0d6046d2dcab.png)\n",
    "\n",
    "Source: https://static.packt-cdn.com/products/9781785888359/graphics/8bffbd94-04f7-46e3-a1e9-0d6046d2dcab.png\n",
    "\n",
    "## Overview of Spark SQL\n",
    "\n",
    "Creating DataFrames\n",
    "- RDD\n",
    "    - createDataFrame()\n",
    "- Text file\n",
    "    - read.text()\n",
    "- JSON file\n",
    "    - read.json()\n",
    "    - read,json(RDD)\n",
    "- Parquet file\n",
    "    - read.parquet()\n",
    "- Table in a relational database\n",
    "- Temporary table in Spark\n",
    "\n",
    "DataFrame to RDD\n",
    "- rdd()\n",
    "\n",
    "Schemas\n",
    "- Inferring schemas\n",
    "    - Why it is not optimal practice\n",
    "- Specifying schemas\n",
    "    - Using StructType and StructField\n",
    "    - Using DDL string (schema = “author STRING, title STRING, pages INT”)\n",
    "- Metadata\n",
    "    - printSchema()\n",
    "    - columns()\n",
    "    - dtypes()\n",
    "- Actions\n",
    "    - show()\n",
    "- Transforms\n",
    "    - select() and alias()\n",
    "    - drop()\n",
    "    - filter() / where()\n",
    "    - distinct()\n",
    "    - dropDuplicates()\n",
    "    - sample\n",
    "    - sampleBy()\n",
    "    - limit()\n",
    "    - orderBy() / sort()\n",
    "    - groupBy()\n",
    "\n",
    "Operations that return an RDD \n",
    "    - rdd.map()\n",
    "    - rdd.flatMap()\n",
    "\n",
    "pyspark.sql.functions module\n",
    "- String functions\n",
    "- Math functions\n",
    "- Statistics functions\n",
    "- Date functions\n",
    "- Hashing functions\n",
    "- Algorithms (sounded, levenstein)\n",
    "- Windowing functions\n",
    "\n",
    "User defined functions\n",
    "- udf()\n",
    "- pandas_udf()\n",
    "\n",
    "Multiple DataFrames\n",
    "- join(other, on, how)\n",
    "- union(), unionAll()\n",
    "- intersect()\n",
    "- subtract()\n",
    "\n",
    "Persistence\n",
    "- cache()\n",
    "- persist(:\n",
    "- unpersist()\n",
    "- cacheTable()\n",
    "- clearCache()\n",
    "- repartition()\n",
    "- coalesce()\n",
    "\n",
    "Output\n",
    "- write.csv()\n",
    "- write.parquet()\n",
    "- write.json()\n",
    "\n",
    "Spark SQL\n",
    "- df.createOrReplaceTempView\n",
    "- sql()\n",
    "- table()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation notes\n",
    "\n",
    "Spark 2.4 is desinged to run on Java 8 (or version 1.8). If you have veriosns of Java greater than Java 8, also install Java 8 and include this export\n",
    "\n",
    "```bash\n",
    "export JAVA_HOME=$(/usr/libexec/java_home -v 1.8)\n",
    "```\n",
    "\n",
    "If using the latest `pyarrow` (`pip install -U pyarrow`), you will need to set this variable\n",
    "\n",
    "```bash\n",
    "ARROW_PRE_0_15_IPC_FORMAT=1\n",
    "```\n",
    "\n",
    "in the file\n",
    "\n",
    "```bash\n",
    "SPARK_HOME/conf/spark-env.sh\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (\n",
    "    SparkSession.builder \n",
    "    .master(\"local\") \n",
    "    .appName(\"BIOS-823\") \n",
    "    .config(\"spark.executor.cores\", 4) \n",
    "    .getOrCreate()    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.0.1'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.get('spark.executor.cores')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.range(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "|  1|\n",
      "|  2|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting data/test.csv\n"
     ]
    }
   ],
   "source": [
    "%%file data/test.csv\n",
    "number,letter\n",
    "0,a\n",
    "1,c\n",
    "2,b\n",
    "3,a\n",
    "4,b\n",
    "5,c\n",
    "6,a\n",
    "7,a\n",
    "8,a\n",
    "9,b\n",
    "10,b\n",
    "11,c\n",
    "12,c\n",
    "13,b\n",
    "14,b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implicit schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (\n",
    "    spark.read.\n",
    "    format('csv').\n",
    "    option('header', 'true').\n",
    "    option('inferSchema', 'true').\n",
    "    load('data/test.csv')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|number|letter|\n",
      "+------+------+\n",
      "|     0|     a|\n",
      "|     1|     c|\n",
      "|     2|     b|\n",
      "+------+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- number: integer (nullable = true)\n",
      " |-- letter: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explicit schema\n",
    "\n",
    "For production use, you should provide an explicit schema to reduce risk of error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = T.StructType([\n",
    "    T.StructField(\"number\", T.DoubleType()),\n",
    "    T.StructField(\"letter\", T.StringType()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (\n",
    "    spark.read.\n",
    "    format('csv').\n",
    "    option('header', 'true').\n",
    "    schema(schema).\n",
    "    load('data/test.csv')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|number|letter|\n",
      "+------+------+\n",
      "|   0.0|     a|\n",
      "|   1.0|     c|\n",
      "|   2.0|     b|\n",
      "+------+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- number: double (nullable = true)\n",
      " |-- letter: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alternative way to specify schema\n",
    "\n",
    "You can use SQL DDL syntax to specify a schema as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = 'number DOUBLE, letter STRING'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_altschema = (\n",
    "    spark.read.\n",
    "    format('csv').\n",
    "    option('header', 'true').\n",
    "    schema(schema=schema).\n",
    "    load('data/test.csv')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(number=0.0, letter='a'),\n",
       " Row(number=1.0, letter='c'),\n",
       " Row(number=2.0, letter='b')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_altschema.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- number: double (nullable = true)\n",
      " |-- letter: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_altschema.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Persist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[number: double, letter: string]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|number|\n",
      "+------+\n",
      "|   0.0|\n",
      "|   1.0|\n",
      "|   2.0|\n",
      "+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('number').show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|index|\n",
      "+-----+\n",
      "|  0.0|\n",
      "|  1.0|\n",
      "|  2.0|\n",
      "+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(col('number').alias('index')).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|  x|\n",
      "+---+\n",
      "|0.0|\n",
      "|1.0|\n",
      "|2.0|\n",
      "+---+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(expr('number as x')).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "|  x|letter|\n",
      "+---+------+\n",
      "|0.0|     a|\n",
      "|1.0|     c|\n",
      "|2.0|     b|\n",
      "+---+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumnRenamed('number', 'x').show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|number|letter|\n",
      "+------+------+\n",
      "|   0.0|     a|\n",
      "|   2.0|     b|\n",
      "|   4.0|     b|\n",
      "+------+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter('number % 2 == 0').show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|number|letter|\n",
      "+------+------+\n",
      "|  14.0|     b|\n",
      "|  13.0|     b|\n",
      "|  12.0|     c|\n",
      "+------+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.sort(df.number.desc()).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|number|letter|\n",
      "+------+------+\n",
      "|   1.0|     c|\n",
      "|   5.0|     c|\n",
      "|  11.0|     c|\n",
      "+------+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.orderBy(df.letter.desc()).show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|  x|\n",
      "+---+\n",
      "|0.0|\n",
      "|2.0|\n",
      "|4.0|\n",
      "+---+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.selectExpr('number*2 as x').show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+---+\n",
      "|number|letter|  x|\n",
      "+------+------+---+\n",
      "|   0.0|     a|0.0|\n",
      "|   1.0|     c|2.0|\n",
      "|   2.0|     b|4.0|\n",
      "+------+------+---+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('x', expr('number*2')).show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sumarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+-----------+-----------+\n",
      "|min(number)|max(number)|min(letter)|max(letter)|\n",
      "+-----------+-----------+-----------+-----------+\n",
      "|        0.0|       14.0|          a|          c|\n",
      "+-----------+-----------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.agg(F.min('number'),\n",
    "       F.max('number'), \n",
    "       F.min('letter'), \n",
    "       F.max('letter')).show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------------+-------------------+\n",
      "|letter|      avg(number)|stddev_samp(number)|\n",
      "+------+-----------------+-------------------+\n",
      "|     c|             7.25|  5.188127472091127|\n",
      "|     b|8.666666666666666|  4.802776974487434|\n",
      "|     a|              4.8|  3.271085446759225|\n",
      "+------+-----------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    df.groupby('letter').\n",
    "    agg(F.mean('number'), F.stddev_samp('number')).show()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Window functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = (\n",
    "    Window.partitionBy('letter').\n",
    "    orderBy(F.desc('number')).\n",
    "    rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+\n",
      "|letter|sum(number)|\n",
      "+------+-----------+\n",
      "|     c|       29.0|\n",
      "|     b|       52.0|\n",
      "|     a|       24.0|\n",
      "+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupby('letter').agg(F.sum('number')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|number|letter|\n",
      "+------+------+\n",
      "|   0.0|     a|\n",
      "|   1.0|     c|\n",
      "|   2.0|     b|\n",
      "|   3.0|     a|\n",
      "|   4.0|     b|\n",
      "|   5.0|     c|\n",
      "|   6.0|     a|\n",
      "|   7.0|     a|\n",
      "|   8.0|     a|\n",
      "|   9.0|     b|\n",
      "|  10.0|     b|\n",
      "|  11.0|     c|\n",
      "|  12.0|     c|\n",
      "|  13.0|     b|\n",
      "|  14.0|     b|\n",
      "+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+\n",
      "|letter|rank|\n",
      "+------+----+\n",
      "|     c|12.0|\n",
      "|     c|23.0|\n",
      "|     c|28.0|\n",
      "|     c|29.0|\n",
      "|     b|14.0|\n",
      "|     b|27.0|\n",
      "|     b|37.0|\n",
      "|     b|46.0|\n",
      "|     b|50.0|\n",
      "|     b|52.0|\n",
      "|     a| 8.0|\n",
      "|     a|15.0|\n",
      "|     a|21.0|\n",
      "|     a|24.0|\n",
      "|     a|24.0|\n",
      "+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    df.select('letter', F.sum('number').\n",
    "              over(ws).\n",
    "              alias('rank')).show()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView('df_table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|number|letter|\n",
      "+------+------+\n",
      "|   0.0|     a|\n",
      "|   1.0|     c|\n",
      "|   2.0|     b|\n",
      "+------+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''SELECT * FROM df_table''').show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------------+-----------------+\n",
      "|letter|             mean|               sd|\n",
      "+------+-----------------+-----------------+\n",
      "|     c|             12.0|              NaN|\n",
      "|     b|              7.5|5.507570547286102|\n",
      "|     a|4.666666666666667|4.163331998932265|\n",
      "+------+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "SELECT letter, mean(number) AS mean, \n",
    "stddev_samp(number) AS sd from df_table\n",
    "WHERE number % 2 = 0\n",
    "GROUP BY letter\n",
    "ORDER BY letter DESC\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## String operatons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split, lower, explode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = spark.createDataFrame(\n",
    "    pd.DataFrame(\n",
    "        dict(sents=('Thing 1 and Thing 2',\n",
    "                    'The Quick Brown Fox'))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|              sents|\n",
      "+-------------------+\n",
      "|Thing 1 and Thing 2|\n",
      "|The Quick Brown Fox|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = (\n",
    "    s.select(explode(split(lower(expr('sents')), ' '))).\n",
    "    sort('col')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|  col|\n",
      "+-----+\n",
      "|    1|\n",
      "|    2|\n",
      "|  and|\n",
      "|brown|\n",
      "|  fox|\n",
      "|quick|\n",
      "|  the|\n",
      "|thing|\n",
      "|thing|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|  col|count|\n",
      "+-----+-----+\n",
      "|thing|    2|\n",
      "|  fox|    1|\n",
      "|  the|    1|\n",
      "|  and|    1|\n",
      "|    1|    1|\n",
      "|brown|    1|\n",
      "|quick|    1|\n",
      "|    2|    1|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s1.groupBy('col').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.createOrReplaceTempView('s_table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------+\n",
      "|regexp_replace(sents, T.*?g, FOO)|\n",
      "+---------------------------------+\n",
      "|                  FOO 1 and FOO 2|\n",
      "|              The Quick Brown Fox|\n",
      "+---------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "SELECT regexp_replace(sents, 'T.*?g', 'FOO')\n",
    "FROM s_table\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numeric operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import log1p, randn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------+------+\n",
      "|number|     LOG1P(number)|letter|\n",
      "+------+------------------+------+\n",
      "|   0.0|               0.0|     a|\n",
      "|   1.0|0.6931471805599453|     c|\n",
      "|   2.0|1.0986122886681096|     b|\n",
      "+------+------------------+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.selectExpr('number', 'log1p(number)', 'letter').show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06913354354513815"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\n",
    "    df.selectExpr('number', 'randn() as random').\n",
    "    stat.corr('number', 'random')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Date and time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = (\n",
    "    spark.range(3).\n",
    "    withColumn('today', F.current_date()).\n",
    "    withColumn('tomorrow', F.date_add('today', 1)).\n",
    "    withColumn('time', F.current_timestamp())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+----------+--------------------+\n",
      "| id|     today|  tomorrow|                time|\n",
      "+---+----------+----------+--------------------+\n",
      "|  0|2020-10-26|2020-10-27|2020-10-26 10:21:...|\n",
      "|  1|2020-10-26|2020-10-27|2020-10-26 10:21:...|\n",
      "|  2|2020-10-26|2020-10-27|2020-10-26 10:21:...|\n",
      "+---+----------+----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = false)\n",
      " |-- today: date (nullable = false)\n",
      " |-- tomorrow: date (nullable = false)\n",
      " |-- time: timestamp (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dt.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting data/test_null.csv\n"
     ]
    }
   ],
   "source": [
    "%%file data/test_null.csv\n",
    "number,letter\n",
    "0,a\n",
    "1,\n",
    "2,b\n",
    "3,a\n",
    "4,b\n",
    "5,\n",
    "6,a\n",
    "7,a\n",
    "8,\n",
    "9,b\n",
    "10,b\n",
    "11,c\n",
    "12,\n",
    "13,b\n",
    "14,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "dn = (\n",
    "    spark.read.\n",
    "    format('csv').\n",
    "    option('header', 'true').\n",
    "    option('inferSchema', 'true').\n",
    "    load('data/test_null.csv')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- number: integer (nullable = true)\n",
      " |-- letter: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dn.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|number|letter|\n",
      "+------+------+\n",
      "|     0|     a|\n",
      "|     1|  null|\n",
      "|     2|     b|\n",
      "|     3|     a|\n",
      "|     4|     b|\n",
      "|     5|  null|\n",
      "|     6|     a|\n",
      "|     7|     a|\n",
      "|     8|  null|\n",
      "|     9|     b|\n",
      "|    10|     b|\n",
      "|    11|     c|\n",
      "|    12|  null|\n",
      "|    13|     b|\n",
      "|    14|     b|\n",
      "+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dn.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|number|letter|\n",
      "+------+------+\n",
      "|     0|     a|\n",
      "|     2|     b|\n",
      "|     3|     a|\n",
      "|     4|     b|\n",
      "|     6|     a|\n",
      "|     7|     a|\n",
      "|     9|     b|\n",
      "|    10|     b|\n",
      "|    11|     c|\n",
      "|    13|     b|\n",
      "|    14|     b|\n",
      "+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dn.na.drop().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+\n",
      "|number| letter|\n",
      "+------+-------+\n",
      "|     0|      a|\n",
      "|     1|Missing|\n",
      "|     2|      b|\n",
      "|     3|      a|\n",
      "|     4|      b|\n",
      "|     5|Missing|\n",
      "|     6|      a|\n",
      "|     7|      a|\n",
      "|     8|Missing|\n",
      "|     9|      b|\n",
      "|    10|      b|\n",
      "|    11|      c|\n",
      "|    12|Missing|\n",
      "|    13|      b|\n",
      "|    14|      b|\n",
      "+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dn.na.fill('Missing').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UDF\n",
    "\n",
    "To avoid degrading performance, avoid using UDF if you can use the functions in `pyspark.sql.functions`. If you must use UDFs, prefer `pandas_udf` to `udf` where possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf, pandas_udf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Python UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf('double')\n",
    "def square(x):\n",
    "    return x**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+\n",
      "|number|square(number)|\n",
      "+------+--------------+\n",
      "|   0.0|           0.0|\n",
      "|   1.0|           1.0|\n",
      "|   2.0|           4.0|\n",
      "+------+--------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('number', square('number')).show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas UDF\n",
    "\n",
    "This can be tricky to set up. I use Oracle Java SDK v11 and set the following environment variables.\n",
    "\n",
    "```bash\n",
    "export JAVA_HOME=$(/usr/libexec/java_home -v 11)\n",
    "export JAVA_TOOL_OPTIONS=\"-Dio.netty.tryReflectionSetAccessible=true\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pandas_udf('double')\n",
    "def scale(x):\n",
    "    return (x - x.mean())/x.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|      scale(number)|\n",
      "+-------------------+\n",
      "|-1.5652475842498528|\n",
      "|-1.3416407864998738|\n",
      "| -1.118033988749895|\n",
      "+-------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(scale('number')).show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grouped agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/spark/python/pyspark/sql/pandas/functions.py:383: UserWarning: In Python 3.6+ and Spark 3.0+, it is preferred to specify type hints for pandas UDF instead of specifying pandas UDF type which will be deprecated in the future releases. See SPARK-28264 for more details.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "@pandas_udf('double', F.PandasUDFType.GROUPED_AGG)\n",
    "def gmean(x):\n",
    "    return x.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------------+\n",
      "|letter|    gmean(number)|\n",
      "+------+-----------------+\n",
      "|     c|             7.25|\n",
      "|     b|8.666666666666666|\n",
      "|     a|              4.8|\n",
      "+------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupby('letter').agg(gmean('number')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark 3\n",
    "\n",
    "Use type hints rather than specify pandas UDF type \n",
    "\n",
    "See [blog](https://databricks.com/blog/2020/05/20/new-pandas-udfs-and-python-type-hints-in-the-upcoming-release-of-apache-spark-3-0.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pandas_udf('double')\n",
    "def gmean1(x: pd.Series) -> float:\n",
    "    return x.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------------+\n",
      "|letter|   gmean1(number)|\n",
      "+------+-----------------+\n",
      "|     c|             7.25|\n",
      "|     b|8.666666666666666|\n",
      "|     a|              4.8|\n",
      "+------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupby('letter').agg(gmean1('number')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grouped map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pandas_udf(df.schema, F.PandasUDFType.GROUPED_MAP)\n",
    "def gscale(pdf):\n",
    "    return pdf.assign(\n",
    "        number = (pdf.number - pdf.number.mean()) /\n",
    "        pdf.number.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/spark/python/pyspark/sql/pandas/group_ops.py:73: UserWarning: It is preferred to use 'applyInPandas' over this API. This API will be deprecated in the future releases. See SPARK-28264 for more details.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|              number|letter|\n",
      "+--------------------+------+\n",
      "| -1.2046735616310666|     c|\n",
      "|-0.43368248218718397|     c|\n",
      "|    0.72280413697864|     c|\n",
      "|  0.9155519068396106|     c|\n",
      "| -1.3880858307767148|     b|\n",
      "| -0.9716600815437003|     b|\n",
      "| 0.06940429153883587|     b|\n",
      "|  0.2776171661553431|     b|\n",
      "|  0.9022557900048648|     b|\n",
      "|  1.1104686646213722|     b|\n",
      "|  -1.467402817237783|     a|\n",
      "| -0.5502760564641687|     a|\n",
      "| 0.36685070430944583|     a|\n",
      "|  0.6725596245673173|     a|\n",
      "|  0.9782685448251889|     a|\n",
      "+--------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupby('letter').apply(gscale).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark 3\n",
    "\n",
    "Use the new `pandas` function API. Currently, type annotations are not used in the function API. \n",
    "\n",
    "See [blog](https://databricks.com/blog/2020/05/20/new-pandas-udfs-and-python-type-hints-in-the-upcoming-release-of-apache-spark-3-0.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `applyinPandas`\n",
    "\n",
    "This implements the `split-apply-combine` pattern. Method of grouped DataFrame. \n",
    "\n",
    "- Variant 1: Function takes a single DataFrame input\n",
    "- Variant 2: Function takes a tuple of keys, and a DataFrame input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gscale1(pdf: pd.DataFrame) -> pd.DataFrame:\n",
    "    num = pdf.number\n",
    "    return pdf.assign(\n",
    "        number = (num - num.mean()) / num.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|              number|letter|\n",
      "+--------------------+------+\n",
      "| -1.2046735616310666|     c|\n",
      "|-0.43368248218718397|     c|\n",
      "|    0.72280413697864|     c|\n",
      "|  0.9155519068396106|     c|\n",
      "| -1.3880858307767148|     b|\n",
      "| -0.9716600815437003|     b|\n",
      "| 0.06940429153883587|     b|\n",
      "|  0.2776171661553431|     b|\n",
      "|  0.9022557900048648|     b|\n",
      "|  1.1104686646213722|     b|\n",
      "|  -1.467402817237783|     a|\n",
      "| -0.5502760564641687|     a|\n",
      "| 0.36685070430944583|     a|\n",
      "|  0.6725596245673173|     a|\n",
      "|  0.9782685448251889|     a|\n",
      "+--------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupby('letter').applyInPandas(gscale1, schema=df.schema).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gsum(key, pdf):\n",
    "    return pd.DataFrame([key + (pdf.number.sum(),)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|letter|number|\n",
      "+------+------+\n",
      "|     c|    29|\n",
      "|     b|    52|\n",
      "|     a|    24|\n",
      "+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupby('letter').applyInPandas(gsum, 'letter string, number long').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, you do not need a UDF in this example!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+\n",
      "|letter|sum(number)|\n",
      "+------+-----------+\n",
      "|     c|       29.0|\n",
      "|     b|       52.0|\n",
      "|     a|       24.0|\n",
      "+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('letter').sum().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So shoudl only be used for truly custom functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(pdf: pd.DataFrame) -> int:\n",
    "    return (pdf.number.astype('str').str.len()).sum()\n",
    "\n",
    "def gcustom(key, pdf):\n",
    "    return pd.DataFrame([key + (func(pdf),)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|letter|number|\n",
      "+------+------+\n",
      "|     c|    14|\n",
      "|     b|    21|\n",
      "|     a|    15|\n",
      "+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupby('letter').applyInPandas(gcustom, 'letter string, number long').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `mapinPandas`\n",
    "\n",
    "This works on iterators. Method of DataFrame. Can be used to implement a filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def even(it):\n",
    "    for pdf in it:\n",
    "        yield pdf[pdf.number % 2 == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|letter|number|\n",
      "+------+------+\n",
      "|     a|     0|\n",
      "|     b|     2|\n",
      "|     b|     4|\n",
      "|     a|     6|\n",
      "|     a|     8|\n",
      "|     b|    10|\n",
      "|     c|    12|\n",
      "|     b|    14|\n",
      "+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.mapInPandas(even, 'letter string, number long').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = 'ann ann bob bob chcuk'.split()\n",
    "courses = '821 823 821 824 823'.split()\n",
    "pdf1 = pd.DataFrame(dict(name=names, course=courses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>course</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ann</td>\n",
       "      <td>821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ann</td>\n",
       "      <td>823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bob</td>\n",
       "      <td>821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bob</td>\n",
       "      <td>824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>chcuk</td>\n",
       "      <td>823</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    name course\n",
       "0    ann    821\n",
       "1    ann    823\n",
       "2    bob    821\n",
       "3    bob    824\n",
       "4  chcuk    823"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "course_id = '821 822 823 824 825'.split()\n",
    "course_names = 'Unix Python R Spark GLM'.split()\n",
    "pdf2 = pd.DataFrame(dict(course_id=course_id, name=course_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>course_id</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>821</td>\n",
       "      <td>Unix</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>822</td>\n",
       "      <td>Python</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>823</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>824</td>\n",
       "      <td>Spark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>825</td>\n",
       "      <td>GLM</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  course_id    name\n",
       "0       821    Unix\n",
       "1       822  Python\n",
       "2       823       R\n",
       "3       824   Spark\n",
       "4       825     GLM"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = spark.createDataFrame(pdf1)\n",
    "df2 = spark.createDataFrame(pdf2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---------+-----+\n",
      "| name|course|course_id| name|\n",
      "+-----+------+---------+-----+\n",
      "|  ann|   823|      823|    R|\n",
      "|chcuk|   823|      823|    R|\n",
      "|  bob|   824|      824|Spark|\n",
      "|  ann|   821|      821| Unix|\n",
      "|  bob|   821|      821| Unix|\n",
      "+-----+------+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.join(df2, df1.course == df2.course_id, how='inner').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---------+------+\n",
      "| name|course|course_id|  name|\n",
      "+-----+------+---------+------+\n",
      "|  ann|   823|      823|     R|\n",
      "|chcuk|   823|      823|     R|\n",
      "|  bob|   824|      824| Spark|\n",
      "| null|  null|      825|   GLM|\n",
      "| null|  null|      822|Python|\n",
      "|  ann|   821|      821|  Unix|\n",
      "|  bob|   821|      821|  Unix|\n",
      "+-----+------+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.join(df2, df1.course == df2.course_id, how='right').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame conversions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([('ann', 23), ('bob', 34)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(rdd, schema='name STRING, age INT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+\n",
      "|name|age|\n",
      "+----+---+\n",
      "| ann| 23|\n",
      "| bob| 34|\n",
      "+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ann', 529), ('bob', 1156)]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rdd.map(lambda x: (x[0], x[1]**2)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ann', 529), ('bob', 1156)]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rdd.mapValues(lambda x: x**2).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ann</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bob</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  name  age\n",
       "0  ann   23\n",
       "1  bob   34"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.toPandas()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
